{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "e367084f-9a92-4932-ac06-fd95d4621058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "import threading, uuid\n",
    "from datetime import datetime\n",
    "from langchain.embeddings import HuggingFaceEmbeddings, SentenceTransformerEmbeddings, OpenAIEmbeddings \n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.vectorstores import Chroma,FAISS\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.document_loaders import TextLoader\n",
    "import re\n",
    "import random, time\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import ChatOpenAI\n",
    "from langchain_core.runnables import RunnableParallel,RunnablePassthrough\n",
    "from operator import itemgetter\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain_community.document_loaders import JSONLoader\n",
    "from langchain_community.chat_models import ChatOpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bcfb88ae-25db-4fb2-9a5e-56beaad7124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_id_from_url(url):\n",
    "    video_id = re.search(r'(?<=v=)[^&#]+', url)\n",
    "    if video_id is None:\n",
    "        video_id = re.search(r'(?<=be/)[^&#]+', url)\n",
    "    return video_id.group(0) if video_id else None\n",
    "\n",
    "def save_transcript_to_file(video_url, output_file, session_id):\n",
    "    video_id = get_video_id_from_url(video_url)\n",
    "    if video_id is None:\n",
    "        print(\"Invalid YouTube URL\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return\n",
    "\n",
    "    local_timestamps = transcripts_with_timestamps[session_id] = {}\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in transcript:\n",
    "            f.write(entry[\"text\"] + \" \")\n",
    "            local_timestamps[entry['text']] = entry['start']\n",
    "\n",
    "def get_timestamp(session_id, sentence):\n",
    "    # Check if the sentence exists in the transcript and print the timestamp\n",
    "    for transcript_sentence, timestamp in transcripts_with_timestamps[session_id].items():\n",
    "        if sentence in transcript_sentence:\n",
    "            #print(f\"A sentence containing '{sentence}' starts at {timestamp} seconds in the video.\")\n",
    "            return int(timestamp)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d6f6b51-b951-42d7-9224-f0d14c00a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transcript():\n",
    "    start_time = time.time()  # Start the timer\n",
    "    session_id = str(uuid.uuid4())  # Generate a unique ID for this session\n",
    "    transcript_file = f\"transcripts/{datetime.now().strftime('%Y%m%d%H%M%S')}_{random.randint(0,100)}.txt\"\n",
    "    video_url = request.form['video_url']\n",
    "    video_urls[session_id] = video_url.split(\"&\")[0]\n",
    "    save_transcript_to_file(video_url, transcript_file, session_id)\n",
    "    print(f\"Save transcript took {time.time() - start_time} seconds to execute.\")\n",
    "    start_time = time.time()  # Start the timer\n",
    "\n",
    "\n",
    "    loader = TextLoader(transcript_file)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split QA texts took {time.time() - start_time} seconds to execute.\")\n",
    "    start_time = time.time()  # Start the timer\n",
    "\n",
    "    with open(transcript_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcript = f.read()\n",
    "    texts_sum = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_text(transcript)\n",
    "    print(f\"Split Sum texts took {time.time() - start_time} seconds to execute.\")\n",
    "    start_time = time.time()  # Start the timer\n",
    "    # Create Document objects from the transcript parts\n",
    "    docs = [Document(page_content=t) for t in texts_sum[:3]]\n",
    "    print(len(texts_sum))\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    docsearch = Chroma.from_documents(texts, embeddings)\n",
    "    qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0), chain_type=\"stuff\", \n",
    "                                     retriever=docsearch.as_retriever(\n",
    "                                     search_type=\"similarity_score_threshold\",\n",
    "                                     search_kwargs={'k':5, 'fetch_k': 50, 'score_threshold': 0.7}\n",
    "                                     ), \n",
    "                                     return_source_documents=True)\n",
    "    \n",
    "    indexes[session_id] = qa\n",
    "    print(f\"QA Embedding took {time.time() - start_time} seconds to execute.\")\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acc2de24-47af-4023-b9d0-4ab09f287667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query():\n",
    "    start_time = time.time()  # Start the timer\n",
    "    \n",
    "    session_id = request.form['session_id']  # The client must send the session ID with each request\n",
    "    if session_id in indexes:\n",
    "        user_query = request.form['query']\n",
    "        output = indexes[session_id]({\"query\": user_query})\n",
    "\n",
    "        result = output[\"result\"]\n",
    "        #print(output[\"source_documents\"])\n",
    "        print(f\"Answering took {time.time() - start_time} seconds to execute.\")\n",
    "        start_time = time.time()  # Start the timer\n",
    "        clip_links = []\n",
    "        for doc in output[\"source_documents\"]:\n",
    "            timestamp = get_timestamp(session_id, doc.page_content[:20])\n",
    "            if timestamp is not None:  # If the timestamp is not None, add it to the timestamp_str\n",
    "                link = video_urls[session_id]+\"&t=\"+str(timestamp)+\"s\"\n",
    "                clip_links.append(link)\n",
    "\n",
    "        return jsonify({'result': result, 'clip_links': clip_links})\n",
    "    else:\n",
    "        return \"No transcript loaded\", 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59175a0a-7c86-42fb-a22d-6ebcc24e20e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dtp6b76pMak'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_video_id_from_url(\"https://www.youtube.com/watch?v=dtp6b76pMak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c65ae73-1b23-4ffd-8e83-bec2a62e6c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = YouTubeTranscriptApi.get_transcript(\"dtp6b76pMak\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5938a48d-a2a3-47bd-849b-68b7fbbdf2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_timestamps = {}\n",
    "with open(\"transcript.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in transcript:\n",
    "            f.write(entry[\"text\"] + \" \")\n",
    "            local_timestamps[entry['text']] = entry['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ac1e9869-8677-4daf-9315-5c3e8a9a53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_file = \"transcript.txt\"\n",
    "loader = TextLoader(transcript_file)\n",
    "documents = loader.load()\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_documents(documents)\n",
    "# print(f\"Split QA texts took {time.time() - start_time} seconds to execute.\")\n",
    "# start_time = time.time()  # Start the timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c18f3968-150f-4967-917c-b6d81732cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(texts, embeddings)\n",
    "qa = RetrievalQA.from_chain_type(llm=OpenAI(model=\"gpt-3.5-turbo\"), chain_type=\"stuff\", \n",
    "                                 retriever=vectorstore.as_retriever(), \n",
    "                                 return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e4343f26-641a-4136-b7e7-5830aba774df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectordb = Chroma.from_documents(texts, OpenAIEmbeddings(openai_api_key=\"\"))\n",
    "\n",
    "template = ChatPromptTemplate.from_template(\"\"\"\n",
    "You are a helpful AI Social media expert. You specialize in YouTube and YouTube keywords. You're an expert at writing descriptions, titles, and you use keywords that help with video discovery. You are given the transcript of a video as context. Answer the questions based on the given context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "fc90a216-be3b-41d7-ac3e-216b1d2d19bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "llm = ChatOpenAI(model_name=\"gpt-3.5-turbo\", temperature=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "38e8a7a0-b9bc-48a0-99ca-b30a8f50f884",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0), chain_type=\"stuff\", \n",
    "                                 retriever=vectordb.as_retriever(), \n",
    "                                 return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "8856d72d-4af4-4e10-841b-18db16cf00d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = qa({\"query\":\" what are the video about\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f287c1bb-7500-4a41-be69-0223a01624fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The videos are about using the Vision Pro.'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "cca47f9e-677c-4e87-8ec1-6e2461621b7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='through a YouTube video.', metadata={'seq_num': 205, 'source': '/Users/junwei/Desktop/InsightFlow/ChatWithVideo/transcript.json', 'start': 480.75}),\n",
       " Document(page_content='through a YouTube video.', metadata={'seq_num': 205, 'source': '/Users/junwei/Desktop/InsightFlow/ChatWithVideo/transcript.json', 'start': 480.75}),\n",
       " Document(page_content='through a YouTube video.', metadata={'seq_num': 205, 'source': '/Users/junwei/Desktop/InsightFlow/ChatWithVideo/transcript.json', 'start': 480.75}),\n",
       " Document(page_content='This video is all about\\nusing the Vision Pro.', metadata={'seq_num': 20, 'source': '/Users/junwei/Desktop/InsightFlow/ChatWithVideo/transcript.json', 'start': 52.38})]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "04c26091-4f30-4ffb-80bb-daebac1747ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b880af69-f346-4486-a302-75e344429d5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "def save_to_json(transcript,filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(transcript, f, indent=4) \n",
    "\n",
    "def load_documents(filepath):\n",
    "    def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "        metadata[\"start\"] = record.get(\"start\")\n",
    "        if \"end\" in record.keys():\n",
    "            metadata[\"end\"] = record.get(\"end\")\n",
    "        return metadata\n",
    "\n",
    "    loader = JSONLoader(\n",
    "        file_path=filepath,\n",
    "        jq_schema='.[]',\n",
    "        text_content=True,\n",
    "        content_key=\"text\",\n",
    "        metadata_func = metadata_func\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "\n",
    "transcript = YouTubeTranscriptApi.get_transcript(\"dtp6b76pMak\")\n",
    "save_to_json(transcript,\"transcript.json\")\n",
    "documents = load_documents(\"transcript.json\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "eb904a0d-a319-421c-94fa-28af2c5c4dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0), chain_type=\"stuff\", \n",
    "                                 retriever=vectordb.as_retriever(\n",
    "                                     # search_type=\"similarity_score_threshold\",\n",
    "                                     # search_kwargs={'k':5, 'fetch_k': 50, 'score_threshold': 0.7}\n",
    "                                 ), \n",
    "                                 return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "909545d5-0c25-4d4d-9dda-907915baccdd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(upbeat music) - All right, so you've seen the unboxing. Now it's time for the breakdown. What is using the Apple\\nVision Pro actually like? This is easily one of Apple's\\ncraziest, most radical, possibly dystopian products of all time. And I have a lot of thoughts here, like I've been using it\\nfor about a week now. There are some parts of this thing that are absolutely incredible, and some other parts that feel weird, or borderline unfinished. There are all kinds of new technologies, from a new operating system\\nto infrared eye tracking to virtually reconstructed\\nversions of you. I feel like there are so\\nmany actually new things that you have to understand\\nin order to get a sense of what this headset\\nactually is and what it does. So I'm gonna break this\\ndown into two parts. This video is all about\\nusing the Vision Pro. It's everything I've\\nlearned from the past week of wearing and getting used to\\nthis thing every single day. But I'm also working\\non a more wide ranging, possibly more existential, review video. But let's just start with the more hardware fundamentals, right? Like what is this thing\\nthat I'm holding literally? Apple Vision Pro at its core, well, it is a VR headset. Now, Apple would never say that, and they probably won't like\\nthat I'm saying that word. You know, I made an entire video about why they refuse to use those words, and they're calling it\\nspatial computing instead. We'll get there. But the truth is it's a\\nreally, really, really high end virtual reality headset. It's something we've seen before, right? It's got displays and lenses and speakers and fans and buttons. And this is a form factor. This is a thing that we have seen before, but before I even turn this thing on, there are clearly several things that are a little\\ndifferent about this one. So first of all, it's made of metal. Lots of metal and glass\\nhere, which are high quality, but heavy materials, relatively speaking. So there's this precisely\\nmachined aluminum frame around the outside. And yes, those are intakes\\nfor fans at the bottom. And then vents for those fans at the top. On the right side,\\nthere's your digital crown that can be pressed in or turned. And then on the other side is\\njust a single larger button. So kind of basically the same\\ntwo buttons as an Apple Watch. And then when you get\\na little further back on this band here, these little pods with downward facing\\ngrills, these are speakers which are pointed straight at your ears, and work surprisingly well. Though of course, it also\\nmeans that people around you can hear a little bit\\nof what you're hearing. There's a little bit of bleed, and I have a lot to say\\nabout spatial audio, so stay tuned for that. But the main event is at the front. There is an enormous piece of glass, which, yes, is very easy\\nto fingerprint and smudge. And then behind that thing,\\nthere's this outward-facing OLED display and a bunch of\\nsensors all the way around, outside facing sensors that go forward, sideways, and straight down. And there's depth sensors,\\ninfrared illuminators, lidar scanners, and just\\nregular old RGB cameras, all being processed by an M2 chip and an R1 chip inside this thing. And then maybe the craziest part, inside the headset, there\\nare a bunch more sensors facing your eyes, tracking\\nyour eyes in real time, for all the eye control and\\neverything that comes with that. And also then to display a\\nrepresentation of your eyes on the outside of the headset. Kinda, we'll get there. But overall, when you put it all together, you get a very well made, very high end, but also pretty heavy\\ncomputer to wear on your face. So officially, this headset\\nwith this solo knit band when I weighed it, showed up as 638 grams, which some of you on Twitter\\nhave already pointed out is actually slightly less than\\nthe plastic Meta Quest Pro. But that Quest Pro also\\nhas a lot of battery on the back of your head as\\na sort of a counterbalance, so the weight distribution\\nis very different. Also, the Quest Pro is not\\nthat comfortable anyway. But the point is this,\\nfor Apple, made the choice of taking the battery off of\\nthe headset, which means okay, now there's nothing on\\nthe back of your head, so you can wear it and\\nlean up against things, and that might be an upside, but that also now means you have to deal with this cable all the time\\nrunning up to your head, and the fact that it's\\nvery front weighted now. All of the weight is on\\nthe front of your face. So this is the battery, as\\nyou saw in the unboxing. If you haven't already seen the\\nunboxing, that just went up. I'll link it below the like button. But this battery is a surprisingly small 3,366 milliamp hours. I say surprisingly small\\nbecause a normal battery bank of this size, you might\\nexpect to be 10, 15, 20,000 milliamp hours. I suspect there's a lot of\\nheat insulation happening here. But it comes with a\\nnon-removable four foot cable, and a proprietary connector\\nat the end of the cable that will twist and lock to the headset. And so the lock is really solid. It makes sense that it's\\nnot just straight USB that could get disconnected easily. Once you connect it, it starts glowing, and then it starts booting up. And there's even a little\\nApple logo that displays on the outside screen\\nwhile it takes, you know, a little under a minute to turn on. So there is no on or off button or switch anywhere on this headset. Maybe kind of like AirPods\\nMax or something like that. So if you ever take the headset off and put it down, it will\\nenter a standby mode after some time, but it won't turn off. If you wanna turn it off,\\nyou literally have to twist and unplug the cable. That's the only way to\\nactually turn the headset off. Now famously already, the battery life with this included\\nbattery, is not super long on this headset. Two to four hours is actually realistic for what you can expect for\\njust like this built-in battery. But that's also kind of right in line with a lot of other VR headsets. Battery life on VR headsets\\nis not that great in general. If you do wanna use it longer,\\nthe only way to do that is there's USBC port on the battery, and you have to plug the battery in. So you could plug the\\nbattery into the wall for infinite battery life, or I guess you could plug it into like a, you could daisy chain another\\nbattery into the other pocket or something for even longer life. But yeah, two to four hours. Now at first it seemed weird to me that the port is on the\\nsame side of the battery as the non-removable cable, but I think it's because\\nthey just want you to default to putting this battery in your pocket, probably in your back pocket. So even if it's plugged into the wall, it can still be in your back pocket. You're just gonna want to\\nget a longer USBC cable. So there are no controllers\\nthat come with this headset. Now it does support other input methods that are like game controllers,\\nand mouse, and keyboard, and those can be incredibly useful, but by default the primary input method for everyone using the Vision Pro is your eyes and your hands. So the first time you put on this headset, it goes through this calibration process, and it's pretty interesting. So the first time you ever put it on, it first adjusts the\\ndistance between the lenses, physically moving them inside the headset to match the distance between your eyes. Then it does this sort of a hand scan so it understands your hands. And then you go through this process of basically looking at a bunch of dots all the way around the screen, and then tapping your fingers\\ntogether to select them. Kind of feels like an\\neye test or something. And then you're in. So first thing you're gonna notice is you can actually kind\\nof put your hands anywhere as long as the headset can see this, just your fingers touching together. So there's a lot of pictures\\nof people using a headset with their fingers, like\\nout in front of them, pinching like that. But you actually don't have to do that. It's such a wide angle\\nbecause of the sensors facing forward and sideways and down. You can kind of just\\nrest your hand anywhere, in front of you, in your lap. As long as you pinch like that, it can generally pick it\\nup, which is impressive. So you're pinching to control anywhere in that 180 degree bubble in front of you. And then the digital\\ncrown, you hit that once, and the app drawer\\ncomes up, pretty simple. Doesn't seem that impressive. But this is actually a peek\\nat the first really impressive thing about this headset to\\nme, which is it seems to have incredible spatial positioning lock, and like, it's really hard\\nto have you appreciate this through a YouTube video. Reviewing VR headsets is hard. But turn around in the room you're in, and picture a wall or a window just appearing locked in place\\nin 3D space in your room, and no matter how much you move your head, or move around, it stays\\nexactly kind of floating where it's supposed to be. But when I say floating, I\\nthink you're picturing like a, a soft float, but it's locked,\\nand that's how it starts. So now you're in Apple's new Vision OS I would describe this as\\nkind of similar to iPad OS, but way more glassy, and of course with the extra dimension of 3D space. So hitting a digital crown\\nwill always get the app drawer back in front of you, and\\nthen simply look at the icon you want and pinch your fingers together, to select it and open that app. Scrolling is basically as you'd expect, you just kind of pinch\\nand grab in the air, and then pull as if it's on a string, and physics let you pull\\nthings through the air. It's pretty intuitive, it's\\nresponsive, it's fluid. Sometimes it's kind of bouncy even. I would say the biggest adjustment is only being able to control exactly what you're looking at. And I don't think people realize how often they're controlling things that they're not exactly\\nlooking directly at with other computers and other UIs. But with this, you can look\\nat the button to select it, and if you look at the\\nnext thing you're gonna do, you're no longer controlling the button. You have to look exactly\\nwhere you're trying to interact with things. It takes a few extra brain cycles to remember to always be looking exactly at the thing you're controlling. So when you open a window\\nof a Vision OS app, like any one of the\\ndefault Apple apps here, it locks into place, it's floating there. It kind of looks, again, like an iPad app, but very glassy, like this\\nfrosted glass around the UI sort of lets you see through a little bit to the color behind it. And it even sometimes casts\\na shadow on the ground in the correct Z space,\\nso it really solidifies that it's floating in front of you. All this makes it feel like the window is in the space around you. Then if you look at the\\nbottom of the window, you get a little bar, you can\\nalways just look at that bar and pinch to drag it around. So drag it forward,\\nbackward, anywhere you want in X, Y, and Z space, and then let go and it just stays absolutely locked. And then you can look at\\neither bottom corner to resize to make it bigger or smaller. And then finally there's\\na little X at the bottom, you select that, that closes it. So that is the basics of Vision OS, and just using an app. Now this entire time, by default, and almost any time they\\ncan, passthrough is on, which means you have the headset on, but you can see with the cameras right through to everything around you. And I think this is\\nwhere Apple really wants to normalize the term spatial computing, because it feels like augmented reality. It feels like you're always\\nable to see the space around you, but technically\\nit's not actually AR, because you are still looking\\nat a reconstructed version through a camera feed\\nof the world around you instead of the actual world around you. But maybe it's all just semantics. I will say, this is the best\\npassthrough of any VR headset I've ever used, and it's\\nnot even that close. Now again, it's so hard to get\\nthis through a YouTube video. It does have screen recording built in, so I'm gonna try to use that. But imagine putting a headset on, and not really feeling like\\nyou're looking at a screen with the real world. Because of the pixel density, because of the 90 hertz refresh rate, and because of the impressive\\ndynamic range of the cameras and the correctly adjusting shutter speed, you just almost don't, you almost just feel like you're looking at the real world, not through a headset. Also the passthrough is\\nso close to real time that I could legitimately\\ninteract with all kinds of things. I could catch items flying at me. I even tried playing ping pong. It was easy, no hesitation. So officially, the R1 chip\\nis doing all the processing of all this stuff and\\nadjusting the shutter speed for different lighting conditions and always keeping passthrough latency under 12 milliseconds, which\\nis the lowest in the industry. But it's really combining\\nthat with how close to reality the colors and brightness\\nand everything are that keeps it feeling kind of real. Basically, the only noticeable restriction is super close up items and\\nobjects can get a bit blurry, and then you can't quite\\nmake out really small or fine texts, so you can't read an email or a tiny text on your phone in your hand, but you can absolutely text people, or read your notifications,\\nwhile keeping the headset on. If you've tried other VR headsets, you know how impressive that is. It's just, it's really good with the tech that exists now for VR headsets. But you can definitely\\nstill take the headset off and be like, oh, it's way brighter in here than I thought it was. Either way, that's all passthrough, but if you ever wanna\\nfully immerse yourself, I mean it is a VR headset after all, all you gotta do is rotate\\nthis digital crown clockwise, just keep turning it, and it will slowly dial your environment more and\\nmore into your field of view until you dial it all the way\\nup to fully surrounding you. So all of the windows\\nyou might have had open will still stay stuck where they were, but everything you're doing\\nis just on the moon now. So yeah, there's a couple environments Apple has built in here, most of them relaxing scenic locations, like in California somewhere, or one really nice one is Mount Hood with a little bit of rain falling. They're not quite photorealistic, but they're just short of photorealistic, like they're the most\\nrealistic digital environments that I've seen. So then the last two big quirks\\nof the UI, control center. So the only way to get to control center is to look up, and you can't just look up, but you have to physically\\nturn your head up and look at this arrow\\nthat appears above you. So once you see that, you select that and then you get your control\\ncenter for things like, you know, battery life and\\nnotifications, focus modes, and screen recording,\\nand pairing to a Mac. But the other big quirk is text input. So you might be wondering\\nhow does text input work with no physical controllers? So there's basically\\nthree ways to do this. So let's say you are in Safari, and you want to go to mkbhd.com. You really want one of those\\nshiny new Chevron hoodies for the rest of winter. Great, how do you do it? So the first way is to\\nliterally hunt and peck poking the keys on the keyboard that appears in the air in front of you. So this one is tough, because\\nit literally only reacts to your pointer finger on each hand. So you actually can't type fast, like with home row or anything like that. Not great. The second way, though, I\\nthink is actually kind of good. It's at least faster,\\nwhich is looking at the key you want to interact with, and\\nthen pinching to select it. So just looking around\\nthe keyboard like this, and selecting the keys. And you might be surprised how\\nfast you can type like this if you actually know your way\\naround a keyboard pretty well. I actually prefer this to\\npoking the virtual keys because I at least get a\\nlittle bit of haptic feedback from my own fingers tapping together. But then in Safari, the last way to do it is literally to just\\nlook up at the microphone and say the URL out loud. MKBHD.com. And then it just hears you and goes to the site pretty quick, if it's a URL that you\\ncan actually say out loud. So, what can you actually\\ndo with this thing? Like now that we know what\\nit is, it's the M2 chip, a computer on your face with the displays and the lenses inside, and all\\nsorts of sensors everywhere. What can this thing actually do? And I feel like the most\\ncommon way to phrase that is what is the killer app? Because that's, we feel\\nlike we need some sort of justification to spend\\nthree, $4,000 on this thing. Like applications made the\\niPhone what it is as we know it, like apps made the iPad. So what is the app\\nsituation on the Vision Pro? So there are two types of apps\\non the Vision Pro, actually. The first is apps that\\nare built specifically for the Vision Pro to take advantage of its awesome experiences. And there are a few of those right now, and then there are all the other apps, which basically are iPhone and iPad apps that happen to be compatible because the developer didn't opt out. And the first kind is way cooler. So these are Apple's stock apps here that come with the Vision Pro. And so these are all, of course,\\nmade just for Vision Pro. So they're gonna have stuff\\nthat takes full advantage of what this thing is capable of. Apple Music is a pretty classic one, like it has all the same functionality of any other Apple Music app, but in this super glassy frosted window, and shows the colors of\\nwhatever's behind it. And you have the sort of sorting\\nmenu on the left hand side instead of across the bottom. That's the basic layout. Same thing with the Notes\\napp and the Settings app. Very glassy, almost looking\\nlike an iPad app in the air, just rebuilt with this\\nnew material design. And then there's the media apps. So Apple TV and Disney+,\\nthey both come pre-installed, which they have built entire environments inside of them for watching media. And there's even a small\\ncollection of videos on the Apple TV app that are shot on a new proprietary format specifically for Vision Pro. So it drops you into a space\\nwith a full 180 degree video, and Alicia Keys walks right up to you and starts singing right to your face. It's crazy. There's also the Photos app, which will let you look\\nat panoramic photos, for example, in this fully immersive view. So you can blow them up to full screen, and then it gives you a\\nbit of a parallax effect around the edges, so it\\nfeels like you're looking into a window of your own\\nphoto and looking around. It's kind of incredible. And then there's also\\nsome other really fun third party apps that I've tried that were built ahead of time. So Sky Guide, this is a good one. You can look around a real\\nrepresentation of the sky around you or any of the\\nconstellations would normally be, you can look at it a little\\nlonger and it'll pop it out. You can pull it outta the sky to get more information about it. It's a pretty great idea. There's another one called Jig Space, which is, it's a sick app, I don't know if I'd ever use it, but basically it lets you load 3D models into the space you're in\\nand mess around with them, take 'em apart, view them in actual size. And this really takes\\nadvantage of how good the placement lock is on the Vision Pro. And you can walk around,\\nand really gets you a better understanding\\nof the scale of things that you don't get to\\nsee up close very often. And then Keynote is another funny one. So you can of course go\\nthrough and edit a Keynote just like normal if you want to, but then they've built\\nthis whole environment for practicing your presentation skills. So you press that and it\\nsays, oh, would you like to go to a conference room, or the\\nliteral Steve Jobs Theater, so you can rehearse\\ntalking to your audience with your Keynote slides behind you. It is genuinely incredibly immersive. And there's already a\\nbunch more apps like this in the App Store already at launch that are specifically\\nbuilt for Vision Pro. So they'll take advantage\\nof its various strengths. Now, are any of these a killer app? Not really. I mean I don't, if you're\\nlooking for any one of these to be the reason why you spend\\nlike $4,000 on this headset, I don't think we have that yet. But then at least there's\\nall the other non-native, but technically still compatible, apps that are in the App Store. And these are gonna look just\\nlike iPhone and iPad apps. Actually, there's a pre-installed\\nfolder on the home screen when you get this thing\\nliterally called Compatible Apps, and there's a bunch of\\nthem from Apple here. They look exactly like iPad apps. I'm surprised actually\\nthat more of them aren't fully built out to take\\nadvantage of Vision Pro, but like, Apple Maps is just the iPad app. And so it would be cool\\nif there were some fun augmented reality overlay\\nwalking directions type stuff, but nope, it's all the\\nexact same functionality that you would find if you\\nopened this app on your iPad. And you can go to the App Store and search a bunch of the names of apps you already know and love,\\nand find them by name and grab them, and they'll\\nwork the exact same way. Crazily enough though, there are already some notable exceptions. No Netflix app for the\\nVision Pro, no YouTube app for the Vision Pro, no Spotify\\napp for the Vision Pro. Apple has kind of a\\ncontentious relationship with a lot of developers right now, especially some of the bigger ones. And so some have made the\\nactive choice to opt out. They're like, we don't wanna be there. This won't be a big enough\\nplatform to matter to us to justify the work. So they're not there. Now I totally get it, but also now as a Vision Pro owner and someone who's using it, I'm like, oh, it's kind of a bummer. I really wanted to be able to\\nwatch a Netflix show offline, downloaded it ahead of time,\\nbut you can't do that now. But at least, at least\\nfor now, for the record, you can use the browser, and anything that would\\nwork in the browser. So if you pull up Safari, and you get a full screen\\n4K YouTube video going, and locked in space, or\\neven in an environment, it looks great. It's razor sharp. Like, I could totally watch\\nYouTube videos like this. But you will definitely\\nbe missing the features of having the dedicated\\napp, like offline video. Honestly to me, the killer\\napp of the Vision Pro isn't just an app, it's\\nactually the ecosystem. And we knew this was coming, but the second you log into a\\nVision Pro with your Apple ID, immediately it starts\\npulling all the services, and all the stuff that you're used to from all the other Apple\\ndevices you already have. And I said this before the\\nVision Pro was announced, I was like, this is the most\\nobvious strategy for Apple because there are lots of people out there who have never considered\\nbuying a VR headset that are considering only this one because they have an iPhone, and this is the one that\\nworks with the iPhone, and none of the others\\nare particularly close. So all of your iMessages are already here, all of your photos are already here and loaded up and backed up. All your Notes are already\\nat your fingertips. You already saw the Keynote app. But okay, easily my favorite feature is connecting to your Mac, right? So anytime your Mac is in front of you and it's turned on, hit that arrow and then there's this little icon to Become My Mac's Virtual Display. So I click that, and then pick my Mac, and it pretty much instantly,\\nit actually blacks out the display of my Mac, and\\nthen turns that display into a 4K window inside of the headset. So now my keyboard and\\ntrackpad still work, even if it is a desktop. The keyboard and the trackpad\\nstill control everything, and you can continue using it\\njust like a normal computer, but with the ability to\\nmake your new 4K monitor as big or as small or close\\nor far away as you want, which is super sick. And then the bonus is\\nyou can still open up and place other Vision Pro\\napps around your Mac computer. So like you can have your\\nMac in the middle here, and maybe you're editing or\\ndoing some work on the Mac app, and then you have a Safari\\nwindow, or Messages, or whatever else you want\\nright next to it around it. And then your keyboard\\nand trackpad can move seamlessly between them\\nall to control all of them. This, to me, as a Mac user,\\nthe ease of use for setup to make this happen, this feels like the biggest game changer, like the most compelling\\nfuturistic feeling use of this headset to me. Especially on a plane. Oh my god, I can't tell you how many times I've had an awkward conversation because, like, I'm editing a video on the plane, the person next to me sees\\nI'm editing a video of myself, and it's kind of weird\\nand hard to explain, but I'm picturing putting the headset on, the display blacks out, but now I can do all the editing I want, and I can make the\\nscreen as big as I want. So I've really enjoyed using that feature. Again, the biggest challenge, though, is still remembering to\\nlook exactly at the thing you want to control. So aside from typing on the real keyboard on whatever window is open, if you want to control something, you\\nhave to be looking at it. Again, it doesn't sound like a big deal, but when you try it,\\nyou'll see what I mean. And then also, odd\\nlimitation, one monitor only. From the Mac, one virtual\\nmonitor only at a time. So if you usually run a\\ndual display setup like I do for Final Cut Pro, big\\npreview on one side, timeline on the other\\nside, you can't do that. You have to use the big one\\nmonitor version of your setup. All right, so you might have realized I've left one thing out this whole time. One thing, you could call\\nit one more thing, sure. It's one more huge crazy thing, but it's kind of the defining\\ncharacteristic of this product and that is Personas. So in all the advertising\\nyou've seen of Vision Pro, there's these eyes on the\\noutside of the headset that looks like they're\\nkind of in a passthrough, like in a dark astronaut\\nhelmet type of thing. Easily the most memed, most unique aspect of this headset, right? It's the only headset\\nwith an outward display. And I mean it's very, very\\nprominent in those videos, but in real life, as you've started to see from some of my footage,\\nit is very different, and I think I figured out why. So first of all, it's not\\nactually see-through, right? There's a whole bunch of computer in between me and you right now. So the eyes aren't on the outside. It's a representation of my eyes based on what all the sensors\\non the inside are seeing. It's reconstructing it on the outside. So those sensors are tracking\\nat 90 frames per second, and they give you optic ID, which is, it's how you log into the\\nheadset and keep things secure. It's basically the same\\nas face ID, or touch ID, it's just looking at and\\nidentifying your eyes. And it also powers the one\\nbeta feature of this headset, which is Personas, which\\nis, it's the most impressive and weirdest thing about this\\nheadset at the same time. I'm calling it right now. So the purpose of the eyes on the outside is really not for you,\\nthe wearer of the headset. In fact, you'll never see it. But it's for the people around you. So when you're in a passthrough mode, your eyes will shine through to indicate that you wearing the headset can see the person outside. So that right there is\\nalready pretty unique. But then, when you're\\nin something immersive and you can't see what's around\\nyou, it covers up your eyes with this sort of like a blue,\\npurple glowing animation. So that intuitively makes sense. You can see the eyes\\nwhen they can see you, you can't see the eyes\\nwhen they can't see you. But crazily enough, there's also a feature where if you have someone\\nwho's outside the headset looking at you, talking to you, and you are in an immersion, but you want to talk to them through that, they will kind of appear through the fog of whatever immersive\\nenvironment you're in. So you just start talking and\\nlooking in their direction. It detects that, and sort of\\nparts a little bit of a fog and that person's eyes\\nwill show through the fog. It's pretty decent. It basically only shows\\none person at a time. And when this is happening on\\nthe outside of the headset, it shows a little bit of\\nyour eyes poking through the purple and blue glow. It's, as you can see, it's all working, but also, I think it looks\\nnothing like the eyes from the ad. So in an effort to make the eyes as presentable as possible, two things. First of all, this\\nscreen is actually behind a lenticular film, which\\nI didn't even realize that from the initial media they had published. But if you've ever heard of that, it's sort of what gives it this 3D depth. You might have seen this on\\nother holographic displays and stuff, but the point of that is to make the eyes appear to\\nbe sunken into the display, like on your actual face, instead of glued to the\\nfront of the headset, which would look a little more weird. But then two, to represent\\nyour actual eyes, they've built in a way to scan in and create a digital\\nrepresentation of your face, which is called your Persona. And it looks like this. So to get those eyes on the outside of the Vision Pro headset,\\nyou have to do something called registering your Persona. This is how it creates\\nthe digital version of you that includes your eyes\\nthat will show up here. So let's do that now. It's actually kind of a cool process. So I'm gonna put it on, and\\nhopefully the screen recording works so you can see\\nexactly what I'm doing. I'll hit the digital crown. I'm gonna go to Settings. And you can do this when\\nyou first set it up. But I'm going to Persona, and\\nI'm gonna hit Get Started. So let's refine my hands real quick. This is capturing detail\\nfrom the front of the headset of the hands in front of me. Once it's done with that- - [Automated Voice] Your\\nPersona, remove Apple Vision Pro. - It's gonna ask me to take it off. So this is how it goes. - [Automated Voice] When you're ready, hold Apple Vision Pro at eye level. Keep your arms and shoulders relaxed. Align your entire face within the frame. - [Marques] My face shows up like face ID. - [Automated Voice] Slowly\\nturn your head to the right. Now slowly turn your head to the left. Now tilt your head up, then tilt your head down. Next, let's capture\\nyour facial expressions. Smile with your mouth closed. Then make a big smile\\nwith your teeth showing. Now raise your eyebrows. Close your eyes for a moment. Capture complete. Put Vision Pro back on to continue. - I will do that. So now I have a menu that\\nsays Creating Persona, and it says it's in beta, and now there's my Persona right there. Kind of uncanny. The hair's a little bit\\ndifferent, but the face. Wow, wow. Okay. So there's different lighting. You can choose it to always\\nbe in studio lighting, and always be in contour lighting. I'll just leave it at\\nnatural, and hit next. You can change the color\\ntemperature of your skin tone. Cool to warm, I think I'm around there. Brightness, darkness. I think I'm around there, near the middle. Next. And then I can add glasses. So if I typically have glasses, which obviously I wouldn't be able to wear in the Vision Pro, you can still\\nlook like you have glasses, anytime you're on that FaceTime call. And then next. Save. And that's it. So I think now you should see my eyes. Maybe. And that that's the\\nthing, it barely shows up. You can barely see my eyes\\nwhen I'm wearing the headset. Now I've tried a couple\\nother scans subsequently, so I've tried different\\nlighting conditions, I've tried different\\nbackgrounds, simple backgrounds, tried different shirts\\nand things like that. It doesn't really ever\\nappear any brighter. I think if you have a\\ndarker skin tone like me, just don't expect the eyes\\nto show up very brightly on the outside of the headset. It's pretty subtle. Even when it does show up,\\nit's a little weird looking. The eyes are a little\\ntoo far apart sometimes. They're a little dim. You see one eye at a time. It's kind of weird. But that Persona though. Whew. That is some pretty interesting stuff. It's crazy that this is actually\\na real thing being shipped, like first Meta started doing it. Now Apple's doing this. This is, again, it's technically in beta. So I dunno, there's room for improvement, but it still works. But as of right now, I feel like this is both incredibly impressive\\nand slightly unsettling. Like, it's very impressive\\nthat this thing, this headset I'm wearing on my face, is tracking all these\\nlittle micro expressions and little movements for\\nmy eyes and my cheeks and my mouth and everything. But at the same time,\\nit's just not quite human. It's right at the edge\\nof the uncanny valley of I'm not looking at a person. So yeah. But the crazy part is you\\ncan now use this Persona as your camera feed for\\nany apps in Vision Pro that require a front facing\\ncamera, like FaceTime. And so I've tried, I've\\nbeen using FaceTime a few times in the Vision Pro, and it is, technically\\nspeaking, incredible. So I've made a few FaceTime\\ncalls in the past few days with some fellow reviewers, who you'll probably recognize\\nfrom their Personas, who are also testing the Vision Pro. And universally, once we\\nall got past the shock of, oh my god, it's you. It looks like a digital version of you. This is crazy. I've never seen anything like this before. Once we got past that, there\\nis a ton happening here. So you can see the FaceTime windows literally appear as just that. They're just like glassy\\nwindows floating in space with people looking through them. And then the angle that\\nyou look into the window is gonna match the angle that\\nthey see you looking at them. Meaning if we're all in\\nVision Pros on this call, unlikely, but hear me out. If we're all in Vision Pros, and you've got a bunch of\\npeople on this FaceTime call, so there's somebody to the left, and somebody to the right,\\nif I look to the person, and make eye contact with\\nthe person to the right, the person to the left\\nsees the side of my head, because I'm looking at somebody else. That's already pretty cool. And then the same thing\\nis true for hand gestures. So we tried this out. Turns out you can reach out and make hand gestures that\\nare tracked by the cameras in this bubble in front of you, and they show up at the correct angle towards the person that\\nyou're gesturing at, so not towards everybody else on the call. Oh wait, wait, wait. Okay, good test. So wait, Justine, do you see this? - Yes. - And Brian, do you see?\\n- I don't see that. I don't see that, Marques.\\n- Whoa. - Now wait. So now Brian, do you see this?\\n(Justine gasps) - Now I can see that, Marques. - And then on top of\\nthat, spatial audio here is incredibly well developed. So again, you're on the\\ncall, the voice of the person to the right comes from the right side. The voice of the person to\\nthe left comes from the left. But also, you can just pick\\nup and move the window around, and that angle will match where\\nthe people are in the room and where their sound\\nand video comes from. If I put you on the\\nother side of the room, it sounds like they're further away. And if I turn up the environment, and bring them into the\\nmoon, or some other 3D space, it actually sounds much more\\nlike I'm in a gigantic space with no echo, versus in the actual room. It's all very subtle,\\nbut very well considered. So once you're in this a while, you start to notice all\\nthese little smaller things. Again, it's not quite human-like. It's not like looking at a\\nvideo feed of a human face, but it is still, like\\nit has a lot of like, this would be the best avatar\\nanyone's ever made in 2K. Like no one's ever done a 2K face scan and had it look this good, but it's still not as\\ngood as a perfect reality. It's a, you've heard the\\nuncanny valley thing before. I think the number one\\nweakness for the avatars or the Personas that I've seen is hair. So basically everyone I've talked to has like a frozen lump of hair instead of flowing realistic hair. And that's true about all flowing things, like however your hair\\nwas when you did the scan, it's frozen that way. And so is any necklace you're wearing, whether it's crooked or not, or I guess, technically\\nalso any makeup you had on, or however you looked\\nwhen you did the scan. Maybe that could be a good thing. Maybe you did a scan when you\\nwere looking all dolled up, and then you get on a 7:00 AM call, and you still look perfect even though you look like you just\\nwoke up in real life. So I guess there's that too. But anyway, all that is to say FaceTime. FaceTime is the most well thought out, like most futuristic\\nVision Pro experience. It just is. So I'll end this video with this. Now you know what it's\\nlike to use and operate the Vision Pro. But there's still a lot more to consider when actually considering\\nif you should buy and own this thing, from the use cases, to the things that work\\nwell, and don't work well, the philosophy behind it, the\\nprices, all of that stuff. That's what's gonna be for my full review. Like there are parts of this thing that are absolutely amazing, unparalleled, best I've ever seen. But the reason it's so interesting is because it's actually a young category. Like we're so used to this\\nslow, boring iteration in mature categories, like\\nsmartphones, and laptops, and you always see the comments talking about how tech is so boring, but now they're actually\\njumping into something risky, and it's actually fun, and\\nthere is downfalls and flaws, and it's fun to actually\\nweigh the pros and cons. So I'll be expanding on all these way more in the full review, but\\nI'll leave you with this. I've got my upsides and\\ndownsides to Vision Pro. It's been a week. Upsides, some of the stuff\\nthat's the best I've ever seen in a headset. Immersion, placement\\nin space, eye tracking and hand control, passthrough, ecosystem, and spatial audio. And the downsides, weight and comfort, the eyes on the outside, app selection right now,\\nbattery life, and price. So the full reviews in the works. Definitely get subscribed\\nto be among the first to see that when it drops. Either way, till the next one. Thanks for watching. Catch you later. Peace. \""
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5903dd1c-4087-4b6a-a097-1dceb055fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcript.txt\") as f:\n",
    "    transcript = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "eaca2b35-d334-4eb8-a795-295a7821789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "t1 = \"Data science is the study of data to extract meaningful insights for business. It is a multidisciplinary approach that combines principles and practices from the fields of mathematics, statistics, artificial intelligence, and computer engineering to analyze large amounts of data. This analysis helps data scientists to ask and answer questions like what happened, why it happened, what will happen, and what can be done with the results.\"\n",
    "\n",
    "t2 = \"Data science is important because it combines tools, methods, and technology to generate meaning from data. Modern organizations are inundated with data; there is a proliferation of devices that can automatically collect and store information. Online systems and payment portals capture more data in the fields of e-commerce, medicine, finance, and every other aspect of human life. We have text, audio, video, and image data available in vast quantities.\"\n",
    "\n",
    "t3 = \"Descriptive analysis examines data to gain insights into what happened or what is happening in the data environment. It is characterized by data visualizations such as pie charts, bar charts, line graphs, tables, or generated narratives. For example, a flight booking service may record data like the number of tickets booked each day. Descriptive analysis will reveal booking spikes, booking slumps, and high-performing months for this service.\"\n",
    "\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "docs = text_splitter.create_documents([t1,t2,t3],[{\"start\":1,\"end\":2},{\"start\":3,\"end\":4},{\"start\":5,\"end\":6}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2535d00f-f9e4-41e5-95e8-a8f3699d722b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Data science is the study of data to extract meaningful insights for business. It is a multidisciplinary approach that combines principles and practices from the fields of mathematics, statistics, artificial intelligence, and computer engineering to analyze large amounts of data.', metadata={'start': 1, 'end': 2}),\n",
       " Document(page_content='This analysis helps data scientists to ask and answer questions like what happened, why it happened, what will happen, and what can be done with the results.', metadata={'start': 1, 'end': 2}),\n",
       " Document(page_content='Data science is important because it combines tools, methods, and technology to generate meaning from data. Modern organizations are inundated with data; there is a proliferation of devices that can automatically collect and store information.', metadata={'start': 3, 'end': 4}),\n",
       " Document(page_content='Online systems and payment portals capture more data in the fields of e-commerce, medicine, finance, and every other aspect of human life. We have text, audio, video, and image data available in vast quantities.', metadata={'start': 3, 'end': 4}),\n",
       " Document(page_content='Descriptive analysis examines data to gain insights into what happened or what is happening in the data environment. It is characterized by data visualizations such as pie charts, bar charts, line graphs, tables, or generated narratives. For example, a flight booking service may record data like the number of tickets booked each day.', metadata={'start': 5, 'end': 6}),\n",
       " Document(page_content='Descriptive analysis will reveal booking spikes, booking slumps, and high-performing months for this service.', metadata={'start': 5, 'end': 6})]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8c256-5d9c-41b2-9b20-925d5f983465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webdev",
   "language": "python",
   "name": "insightflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
