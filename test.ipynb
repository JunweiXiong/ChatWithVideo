{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e367084f-9a92-4932-ac06-fd95d4621058",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, render_template, request, jsonify\n",
    "import threading, uuid\n",
    "from datetime import datetime\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import re\n",
    "import random, time\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_openai import ChatOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcfb88ae-25db-4fb2-9a5e-56beaad7124a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_id_from_url(url):\n",
    "    video_id = re.search(r'(?<=v=)[^&#]+', url)\n",
    "    if video_id is None:\n",
    "        video_id = re.search(r'(?<=be/)[^&#]+', url)\n",
    "    return video_id.group(0) if video_id else None\n",
    "\n",
    "def save_transcript_to_file(video_url, output_file, session_id):\n",
    "    video_id = get_video_id_from_url(video_url)\n",
    "    if video_id is None:\n",
    "        print(\"Invalid YouTube URL\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return\n",
    "\n",
    "    local_timestamps = transcripts_with_timestamps[session_id] = {}\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for entry in transcript:\n",
    "            f.write(entry[\"text\"] + \" \")\n",
    "            local_timestamps[entry['text']] = entry['start']\n",
    "\n",
    "def get_timestamp(session_id, sentence):\n",
    "    # Check if the sentence exists in the transcript and print the timestamp\n",
    "    for transcript_sentence, timestamp in transcripts_with_timestamps[session_id].items():\n",
    "        if sentence in transcript_sentence:\n",
    "            #print(f\"A sentence containing '{sentence}' starts at {timestamp} seconds in the video.\")\n",
    "            return int(timestamp)\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6d6f6b51-b951-42d7-9224-f0d14c00a787",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_transcript():\n",
    "    start_time = time.time()  # Start the timer\n",
    "    session_id = str(uuid.uuid4())  # Generate a unique ID for this session\n",
    "    transcript_file = f\"transcripts/{datetime.now().strftime('%Y%m%d%H%M%S')}_{random.randint(0,100)}.txt\"\n",
    "    video_url = request.form['video_url']\n",
    "    video_urls[session_id] = video_url.split(\"&\")[0]\n",
    "    save_transcript_to_file(video_url, transcript_file, session_id)\n",
    "    print(f\"Save transcript took {time.time() - start_time} seconds to execute.\")\n",
    "    start_time = time.time()  # Start the timer\n",
    "\n",
    "\n",
    "    loader = TextLoader(transcript_file)\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    texts = text_splitter.split_documents(documents)\n",
    "    print(f\"Split QA texts took {time.time() - start_time} seconds to execute.\")\n",
    "    start_time = time.time()  # Start the timer\n",
    "\n",
    "    with open(transcript_file, \"r\", encoding=\"utf-8\") as f:\n",
    "        transcript = f.read()\n",
    "    texts_sum = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=0).split_text(transcript)\n",
    "    print(f\"Split Sum texts took {time.time() - start_time} seconds to execute.\")\n",
    "    start_time = time.time()  # Start the timer\n",
    "    # Create Document objects from the transcript parts\n",
    "    docs = [Document(page_content=t) for t in texts_sum[:3]]\n",
    "    print(len(texts_sum))\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    docsearch = Chroma.from_documents(texts, embeddings)\n",
    "    qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0), chain_type=\"stuff\", \n",
    "                                     retriever=docsearch.as_retriever(\n",
    "                                     search_type=\"similarity_score_threshold\",\n",
    "                                     search_kwargs={'k':5, 'fetch_k': 50, 'score_threshold': 0.7}\n",
    "                                     ), \n",
    "                                     return_source_documents=True)\n",
    "    \n",
    "    indexes[session_id] = qa\n",
    "    print(f\"QA Embedding took {time.time() - start_time} seconds to execute.\")\n",
    "    start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "acc2de24-47af-4023-b9d0-4ab09f287667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query():\n",
    "    start_time = time.time()  # Start the timer\n",
    "    \n",
    "    session_id = request.form['session_id']  # The client must send the session ID with each request\n",
    "    if session_id in indexes:\n",
    "        user_query = request.form['query']\n",
    "        output = indexes[session_id]({\"query\": user_query})\n",
    "\n",
    "        result = output[\"result\"]\n",
    "        #print(output[\"source_documents\"])\n",
    "        print(f\"Answering took {time.time() - start_time} seconds to execute.\")\n",
    "        start_time = time.time()  # Start the timer\n",
    "        clip_links = []\n",
    "        for doc in output[\"source_documents\"]:\n",
    "            timestamp = get_timestamp(session_id, doc.page_content[:20])\n",
    "            if timestamp is not None:  # If the timestamp is not None, add it to the timestamp_str\n",
    "                link = video_urls[session_id]+\"&t=\"+str(timestamp)+\"s\"\n",
    "                clip_links.append(link)\n",
    "\n",
    "        return jsonify({'result': result, 'clip_links': clip_links})\n",
    "    else:\n",
    "        return \"No transcript loaded\", 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890e9546-325e-4ea1-95ce-cd2fc58417ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ed244c9-9e27-4f7e-be19-1efdc8096351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'7bde9362-4aed-40ad-a505-8410f8d3449a'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session_id = str(uuid.uuid4())\n",
    "session_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59175a0a-7c86-42fb-a22d-6ebcc24e20e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dtp6b76pMak\n",
      "XxOh12Uhg08\n"
     ]
    }
   ],
   "source": [
    "video_URLs = [\"https://www.youtube.com/watch?v=dtp6b76pMak\",\"https://www.youtube.com/watch?v=XxOh12Uhg08\"]\n",
    "video_IDs = []\n",
    "for url in video_URLs:\n",
    "    id = get_video_id_from_url(url)\n",
    "    video_IDs.append(id)\n",
    "    print(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7c65ae73-1b23-4ffd-8e83-bec2a62e6c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "transcripts = []\n",
    "for id in video_IDs:\n",
    "    transcripts.append(YouTubeTranscriptApi.get_transcript(id))\n",
    "print(len(transcripts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5938a48d-a2a3-47bd-849b-68b7fbbdf2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_timestamps = {} #key session, value {key:video_id, value{key: text, value:start_timestamp}}\n",
    "\n",
    "# create directory\n",
    "directory = f\"transcripts/{session_id}\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "# cobine the transcripts\n",
    "for transcript,video_id in zip(transcripts,video_IDs):\n",
    "    with open(f\"{directory}/{video_id}.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        local_timestamps[video_id] = {}\n",
    "        for entry in transcript:\n",
    "            f.write(entry[\"text\"] + \" \")\n",
    "            local_timestamps[video_id][entry[\"text\"]] = entry['start']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac1e9869-8677-4daf-9315-5c3e8a9a53d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=0)\n",
    "\n",
    "documents = []\n",
    "for video_id in video_IDs:\n",
    "    file_path = f\"{directory}/{video_id}.txt\"\n",
    "    loader = TextLoader(file_path)\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "# Split the documents into smaller chunks\n",
    "texts = text_splitter.split_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8e4089d6-d6f1-4173-8812-4b48f5126cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test manully \n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=3000, chunk_overlap=0)\n",
    "\n",
    "documents = []\n",
    "for file_path in [\"interview1.txt\",\"interview3.txt\"]:\n",
    "    loader = TextLoader(file_path)\n",
    "    documents.extend(loader.load())\n",
    "    \n",
    "# Split the documents into smaller chunks\n",
    "texts = text_splitter.split_documents(documents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4f302367-bcfb-49b8-a76b-35f985a33393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"Cool. And yeah, just the first question is, I'm curious to know, I know that you are UX researcher yourself and I'm wondering what is your process of doing research in your work? My process is I start a meeting, I record it on Zoom as well as kind of how we're doing today and then I usually go through, I organize all my files by participant, by date, I have video recording and then I also transcribe the audio file so I have it in text format as well to easily be able to review. And then from there I usually add any insights that I see either by re-listening to the recording or by viewing the transcription into a digital affinity map and I generally use Figma for that and that's how I kind of am able to organize my findings and put them into a report that I then deliver to stakeholders. Okay, got it. So you said that you will use another software for transcribing, right? Do you know, could you give us an example of what software that you use for transcribing? Right now I'm using Microsoft to transcribe. You can do it easily with Word, it's not perfect but it's still a lot better than doing by hand. Yeah, definitely got it. And so after the transcribing you will have the interview scripts in Microsoft Word and then you will manually highlight those to find a key important thing to it. How do you categorize those insights or find the insights? Yeah, sometimes I will highlight them if I want to come back to it. But generally once I have the transcription and I go through and I kind of clean up the file a little bit, I just kind of reread through it. And then the important points that I see I copy into like an affinity map that I kind of created like a digital one in Figma where I have like individual post-its with text and that's where I kind of copy everything and I color code by each participant is a different color so that way I can see the range of responses I'm getting. If I get a lot of people saying the same thing, I can kind of get a visual aid in seeing the range of people that are saying similar things. Okay, I see. And yeah, I know I was wondering what specific tasks or activity that during this user research analysis process that you found is most time consuming or tedious. I would say the transcribing is the most tedious just because because the software I use is very imperfect. There will be a lot of mistakes. It will hear a word that's incorrect. I have to go through and make sure it doesn't say something really weird. Sometimes it says really weird things. For example, the research project I'm doing right now, you have to take a scan but oftentimes the transcription will transcribe it as skin and I'm like, no, we're not dealing with skin, we're doing a scan. So it's like just making sure that it's not completely bizarre. Yeah, understandable. So how many percentage of adjustment that you usually need to make after the transcribing from the Microsoft? I would say probably 20 to 30 percent depending on how nitpicky I am that day. Some\", metadata={'source': 'interview1.txt'}),\n",
       " Document(page_content=\"days I'll, I mean, there's a lot of grammatical things like it'll put periods and question marks where they don't belong. But depending on how nitpicky I'm feeling that day I might just leave them there because the gist of the text is still there, the punctuation is less important. It's more for my reference than anybody else's. So it doesn't need to be perfectly clean. Got it. Got it. And yeah, and besides that is there any other time-consuming tasks that you can think of? I mean, the affinity mapping does take a little bit of time but it doesn't feel as time-consuming as the transcribing and pulling the findings is the most time-consuming thing. Got it. And so for the pulling of findings, you mean like highlighting those things to find and copy and paste those sentences to the stigma? Yeah, I would say so. Okay. Mostly because my tests are generally a little bit longer. Like sometimes they run over an hour. I try to keep it under that but sometimes it's difficult especially if there's technical difficulties, if the user is having a hard time with certain tasks or if they just have a lot to say on the subject, these are things out of my control as a researcher. So I've had tests go on over an hour and I'm like, sorry. Yeah, I mean, I am a UX researcher myself actually and I totally feel the same way. Yeah, so like sometimes we have like 10 interviews and each interview you have 40 to sometimes last until one hour. It's really long transcripts for us to read again and summarize all those insights. So yeah, yeah. And you mentioned for the affinity mapping is one of the way that you will use it as a user to present to the other teams, right? I generally don't show them my affinity map. I generally put it together into a report but like for time constraints, I have shown like if I'm seeing, for example, one of our biggest, I guess, goals for the product that we're creating is bringing value and also earning trust of users. So I remember pulling those specific insights from my usability tests and showing my stakeholders. This is where people are finding the most value and this is how we can earn their trust. So those are like very important factors that I am identifying in my test. So I'll show them in my affinity map, the varied responses that we're getting because I can deliver a report and say people value is important and this is what people are finding valuable. But sometimes it helps them to see the variation in different responses between participants of different demographics. Yeah, definitely got it. So I was curious to know which software that you use to write the report. Do you really think, oh, okay, got it. Yeah, if I'm trying to do something really quickly, I might use PowerPoint just because it's already set up and I just input text, but generally I use Figma because it's a little more flexible. Okay, got it. So you use Figma for affiliate mapping and also create a report using Figma. Okay. And yeah, I'm also curious, besides a\", metadata={'source': 'interview1.txt'}),\n",
       " Document(page_content=\"Philippine mapping, is there any other way that for you to visualize your insights? I mean, for example, the video clips is that your company sometimes wants you to show the video edited video clips of the interview, interviewees, or just be fine. I upload all of the videos. So to our company wide server, so I will have team members sometimes go and watch a video. If I tell them something about it and they're like, oh, that's interesting. I want to go watch. Like, I will have sometimes my stakeholders will sometimes go back and watch recordings to get a little more information. But generally, I just include screenshots and visuals that will help them understand the context of what's going on for a specific insight. Okay, got it. And you mentioned this. This is for user interview or for user testing or for both for both. Like, this whole process for both. Yeah, because I've done user tests. I've done interviews, but I've also done studies that kind of take both into accounts, like half and half, half user testing, half interview. Yeah, totally understand. And for the user interview, so I will imagine, is it like a moderated user interview? Like, you will lead the user testing where you are moderated user testing, like you just let users to finish order tasks themselves. And then generally do moderated. I haven't done any unmoderated yet, although that is something that I've been looking into just to get a wider breadth of feedback. Yes. Okay, got it. And, um, yes, and I also curious, yeah, besides the fitting mapping, do you also need to create persona sometimes to present your findings or summarize your results? I have for discovery, like early discovery type projects. I have done some demographic research and created personas, but they're more proto personas than actual personas, if that makes sense, it's all it's mostly based on research and assumptions, not based on our actual users. Okay. Oh, God. And, um, yeah, and also besides besides user interviews and testing, also curious, you know, do you also involve in some survey analysis in your work as well? For example, for the open some open text questions in the survey, how do you usually analyze that? We have a feedback survey that we've been sending, and we ask for numerical ratings. So that'll just give us a little bit of a very easy way to say, oh, we're getting an average low rating or average high rating, or where can we like, and then they'll have open any questions about where we can improve. But our, our, our outreach for surveys isn't super, super high, so we haven't really had to do any detailed analysis of that yet. Okay. Got it. And, um, yeah, that was really helpful. And I wonder, uh, for the collaboration process, um, do you, like, how, how do you usually collaborate with the other designers or product managers? And do they wait until you finish the whole research project and checking what your progress or they will, like, ask you during the process when you do in the research?\", metadata={'source': 'interview1.txt'}),\n",
       " Document(page_content=\"Um, a little bit of both. I'll generally show them my test plan and my script ahead of time, and I'll get feedback live during our, our meetings, um, on how they kind of envision and what, if their goals differ from my goals, because I kind of have to work up front based on what I know and the assumptions that I make, but sometimes there's additional information that I don't have. So that's helpful to have them input their feedback in real time. And then I've also, like, as our product has evolved, I've gotten, um, information from my stakeholders saying, oh, we'd really like for you to add a question about this or things, things of that nature. So I'm, I'm able to be flexible and make changes accordingly. And it's fine. It's fine. I get a lot of like, we're very small team. So we're very much involved in every step of the process. Um, so I actually enjoy having my stakeholders give me feedback, which I know for some researchers and designers, that's not so great. Um, but because I feel part of the team and it very much involved in the process from start to finish, it's like more, maybe it's more rewarding for me because of that. I don't know. Got it. Yeah, that sounds like the very, um, close connected collaboration relationship in your team. And I wonder so how, like, when they give you feedback, is it normally in the meeting or they will just comment in the document or infigma to give you the feedback? I get both. I get a lot of a little bit of both. Um, we use box and I'll usually upload my test plan and my script to box. It's like our server network or our storage network. It's kind of like a drop box type of situation, but we're all in it. We all have access to it. So sometimes my design lead might give me comments, like, for small changes, like, oh, maybe add this here or something like that. But then oftentimes if we, if we have the time, we'll just discuss it over a Zoom call. Okay, cool. Got it. And the box that the, the software that you just mentioned, is it like, software for you to store also for you to store all the previous documents that other team members can always check? Yeah, everything is there from previous research studies. Um, okay, all aspects. It's, it's, we treat it as like a non, like a server, but it's really just a storage um, software. Yeah. Got it. Yeah. So according to what you just mentioned, you, in your team, you use Figma, you use box, you use Microsoft Word, say any other software that you, you get, your team will use. Zoom. Zoom is a big one. Okay. Yeah. Zoom, right. And then as far as online, do you want to know about like online programs that we use as well? Because we have like Notion and AirTable. Oh, yeah. So these are also used within the product teams, right? Notion and and air table. Yeah. Yeah. Table. Okay. I'm very, I don't even know a lot about our table. Do mine tell me a little bit what is that useful? Yeah. It's very similar to how we use Notion, but Notion, I think is more for the engineers\", metadata={'source': 'interview1.txt'}),\n",
       " Document(page_content=\"and air table is more just for product development. So we just input feedback, we input things to be addressed in order to grow the product and improve it, things like that. And it kind of shows like a in progress, what's priority zero priority one, which things need to be addressed immediately versus more as a nice to have. Okay. Okay. Yeah. That's really, really helpful. And I think I have one, a tool for your two last questions. And that would be the end. So another question is, I wonder in your team, who is usually responsible for making decision of buying the software or not? Oh, probably the co founders. I mean, the zoom account, like I use that on my personal and the Microsoft, I use that for my, I use my personal for both of those. But a lot of our meetings are also held over zoom. I'm not sure what the other team members might use on their own personal side. But we, because we're a startup team, we don't have like designated, except for like the things that we share information on like box and air table, everything else is kind of like, in person you'll lose. Yeah. Loosely, loosely designated. Yeah. Got it. Yeah. And may I ask how many UX researchers seeing your team right now? And also, you ask these owners? I have, I'm the researcher. And then there is a design lead, who's kind of my direct report. And I work with her also on some design projects, because I, we're, we're a small team. So we kind of have to be all hands on deck a lot of the time. But it's fun. It's a, it's a great way to learn a lot. So it's awesome in that sense. Yeah. Yeah. Because I was working as a small company before. So yeah, I, I totally feel that I'm also the one with only one UX researcher in the team back then. Yeah. And yeah, just at the end, I just want to briefly just talk about our idea and a little bit and see how much that you were interested in this idea. So right now, our thinking is that we want to simplify the process of manually tagging those interview scripts and categorize those insights for you using this AI technology. So what we are imagine is after you upload your interview, zoom video to our website, you will, you will be like, you can enter your research questions and we can help you generate, generate all those insights with the evidence with the quotes from the interview scripts. And you can choose different visualization way that maybe you want affinity mapping and we will, we'll pull out this affinity mapping and fit all those insights into those affinity mapping framework. And if you want just points or just text, we can also do that for you, where some people, they want video clips, they want to show like, add all those supported evidence into one video clip that we can also have that option for them. So yeah, I'm just curious to know how do you find this will be helpful for you in this in your research process. They can just be honest that give any feedback that you can think of. Yeah, I mean, I actually, I saw another software that\", metadata={'source': 'interview1.txt'}),\n",
       " Document(page_content=\"someone was launching that's kind of similar where it has the video, it pulls out the insights and then you can also go through the script, the text and highlight additional things that you think are interesting because I mean, AI does its best, but it doesn't have the human touch, right? I'll tell you the name of it. It was called, I was just looking in, it's called Envision, but it's spelled, are you familiar with it? I have a card of it. Is it, is it an V.I? I just, I copy the name and chat for you. So maybe it might be helpful to check it out since you're still in discovery. Yeah. And it was nice. It was organized. It had the video, it had the transcription already ready to go, but I think that, that human element for me was still missing. So I still prefer to go through and do it myself, maybe because I don't trust the insights that it's pulling as much as I trust my own brain. But I did like that it would automatically, once you uploaded the video, it would automatically come create the text file and highlight things that it thought were important to the research that would be useful for the researcher and for the team to see. Wow. And I know there's like other one, another one that somebody showed me, but I haven't used it yet. The problem is, is a lot of these are paid accounts. They're not free. So I don't really feel the need to spend the additional money. Maybe if the team grows and we can add something like that to the company repertoire, that would be nice. But yeah, I am trying to think of how like, I think having the affinity map feature would be nice as well, just because then you can, you can map all the insights by theme and play around with it in your own, because like maybe there's something a little bit deeper that you're trying to identify that's not on the surface, that you kind of have to look a little between the lines of what the speaker is, or what the participant is saying. Maybe there's like having the video is helpful because maybe what they're doing is also important. Like for example, my research study, when you, you take a body scan. So part of me, part of the research study is just seeing how people interact with the scanning technology. And if they're able to like, there's a different, there's a difference between people who may be more physically coordinated than not. So that's helpful for us as a team to identify where people are struggling and how we can make it easier and more user friendly. It's like a new piece of technology. Like a lot of people are really unaccustomed to having to like, do the pose and the spin and it's a little, some people, they get it right, right away and other people, they struggle a little bit. So it's interesting. Yeah, so it is not sometimes it's not all about the, what you said is also about how they react, how they move. I mean, during the user interview, what they click. Body language and that sort of thing. Like you can tell if somebody's getting frustrated, even if they\", metadata={'source': 'interview1.txt'}),\n",
       " Document(page_content=\"don't say it. Yeah, yeah, definitely. So yeah, I guess the human part is still very important, especially analyze those qualitative data. So, and for the software that you just mentioned, invention, I think their idea is really, very similar to what we're trying to do. So definitely you probably will be a direct competitor for that. So I will check it out later. It's not only that, that's very helpful. We haven't noticed the software before and yeah, just trying to distinguish ourselves among all those others. Software, because AI kind of like involved in every software right now, every software trying to have this AI feature. So we're also thinking besides AI feature, what is our strength that could make us stand out than the others? Yeah, absolutely. I think, yeah, I would say being able to kind of map the insights would be kind of a cool way. Because I don't remember if they had that feature yet, I think they just had the video and transcribed and like highlighting the insights. And then you could, you could like go to a page where say you wanted to see, they had like chips of what insights they identified, or what insights like you could add themes. So you could select that theme and then all of the things that were tagged with that theme would come up. But it's still very much like a wall of text, not very visual. So I think, yeah, I think, yeah, I think depending on the person research design, some people may prefer having text and be more analytical. But I think from a design perspective, having those visual elements is important. Yeah. Okay. Cool. Yeah, that is probably the things that we could do. I focus on the visualizing. Yeah, thanks. And I'm also curious like, how long do you usually spend on those like tagging and analyzing those interviews? In general, I mean, the whole part, the whole research part, not only just wanting to read the whole interview part. Oh, I couldn't even tell you to be honest. Because I could spend a couple of hours just on one participant, depending on if I'm, if I'm on a tight deadline, I might just skim through and like physically write down the things that I think are most important. Like if it's like a major usability error, I may just write it in a notebook. And then put together a quick report saying like, Oh, these are things we need to address right away. But the study I'm doing now, I've been doing for like two months now. So it's gotten, because we're, we're continuously iterating on a project as we continue testing. So I have been more focused on testing than I have been on pulling my fine days. So I'm actually a little bit behind on my documentation. Yeah, got it. Yeah, it's totally understand. So also, I'm just wondering. Yeah, sorry, I just lost my question. Let me think. Yeah, sorry, I was about to have a question just a minute. Just totally forget that. Um, yeah, I think that's all the questions. I guess if I have, if I remind my question, I probably will send a message to you in Slack. Yeah,\", metadata={'source': 'interview1.txt'}),\n",
       " Document(page_content=\"I'm sorry that I my brain was like, no, that's okay. It happens. I'm happy to answer any additional questions. And if you guys still, if you need testers in future to like try prototype, I'm totally open to that. Please, please feel free to reach them. Yeah, that would be so nice. And if you know anyone who also, who is also one of also working industry as a UX researcher or product designer or product manager, and they're interested in doing this user interview, feel free to introduce them to me. Yeah, we are really, we really need a lot more participants right now. So yeah, thanks. Well, I know you posted in, um, what Slack channel did you post in? Oh, I posted the personal project help, I think. And was it in tech? Was it in tech fleet? Yeah, yeah. Okay, there's also another, um, there's a couple other Slack networks that I'm a part of. One of them is called hexagon. Are you familiar with it? No, how, I do know how can I join this channel? I can try to invite you to, actually, I might be able to invite you. Um, I'll see what I can do. Okay. Yeah, because, so is this also like a community for the product teams? Yes. And I've, I've found, um, I've used it to find participants for my own research study as well. So it's, there's researchers on there. And if you say like this is specifically to UX research, I think there are people. In fact, I think I might know somebody who'd be interested. I'll reach out to her and see. Oh, that'd be awesome. Yeah. Thanks so much. Yeah. Just let me know who sent me my sessions, like if she would be down for interview. Yeah. And you, you just remind me of the question that I was attending to us. Oh, yeah. Yeah. I was trying to ask you for the recruiting part. Um, so you mentioned that you use this, um, channel Slack channel to recruit participants. What are the other channels do you use to recruit? Um, do you find challenging? Recruiting can be tough. Yeah. It's a full-time job. Um, oftentimes helping other people with their tests, I can get recruit, uh, I can recruit participants that way. I also just post, I post on tech fleet. I post on hexagon and I post on my student, um, Slack channel, um, which I don't know if I, unless you were a student, I don't know how you'd be able to get into that one, unfortunately. Um, and those are my main avenues. And then my personal network, of course, like I had an old coworker that recently I learned that she was having a hard time finding genes. And I'm like, oh, you'd be a perfect participant for my test. And she's like, yeah, let's do it. Got it. And, um, so do those usually come with the incentives where it's usually free of like voluntary. It's usually voluntary. Um, I do try to offer people something in return. Like, I had a girl who just asked me to give her feedback on her portfolio. And I was like, absolutely. Um, sometimes, you know, my friends, I'm like, I owe you a cup of coffee. And they're like, nah, don't worry about it. Cause they're my friends and they'll\", metadata={'source': 'interview1.txt'}),\n",
       " Document(page_content=\"just do it because it's fine. And then as far as like from the student network, usually if they're doing usability tests or their students and they want advice on, like I have a call later today with someone who's interested in learning more about UX research. And I might just say, Hey, you're, would you like, I might, I'll probably ask you to like, I ask people if they'll participate in my test. And sometimes they do. And sometimes they can't. And that's fine. It's just a way to like help, help everybody. And also hopefully, you know, we all help each other, right? Because we're all researchers. Yeah, it's really, yeah. Like any testing that you need participant, I'm also we're going to join. Yeah, I'm really thankful that you, your, your insights today, I think it's really helpful. And you mentioned several stuff, software that I definitely need to check it out later. So yeah, yeah, I'm happy to help. And yeah, I'll send you some information about my test. If you like to participate or it's up to you, it's not required, obviously, because I'm happy to help regardless. But I appreciate the offer. Yeah, it would love to. And I think that would be the end of all the questions we have at the end of this interview. So, since again, and I'm really enjoyed chatting with you and I will keep you updated of our progress in this project. Yeah, please do. I'm excited to see what you guys come up with. Yeah, excited. And then have a great day, thank you, you too. Bye, Catherine. Thank you.\", metadata={'source': 'interview1.txt'}),\n",
       " Document(page_content=\"record. Yeah, so yeah, I just finished my introduction. So do you want to like briefly introduce yourself and also like your role, work as a UX designer before? Because I think I'm mostly interested in your experience working as startup. So maybe you can tell a bit of, of your, yeah, yourself and your experience. Okay, so I'm Zoe and I currently I work as a part time designer, product designer in a startup and then the startup, the product is on building a platform. It's basically a dating app and the market is mostly in Asia. And previously before I graduate from college, I also work in two startup. And one is not a startup, but the tech team is a new, really new team. So I'm the only one designer. And before that, I also worked in another startup that they, their products, their product was building a platform for parents to find offline events for their kids. And their kids are, their ages are mostly around like five to 12. So it's pretty young kids. And we built that platform and I was the, you also product design intern at the time. And there was another product design lead. And only three of us. Another one is the UX researcher. So three of us working, what worked in that startup. But unfortunately, while I was interning the startup just closed. In that one, I think for that, my, it was, it was my first experience as a product design intern in the whole industry. And I did a lot of research in that interns, I can share more, because currently in my current startup, it's very, very new. And we had, we didn't add a lot of resource. So now we mostly doing the branding stuff. So I didn't do research in this company. So I can share more for my first one. Yes. So you said you, you intern at three companies, like as designers, right? So yeah, in general, a time yes. Yeah, right. So like, what is the company size for those three company and for the company that you conduct research? Especially like, what is the company size for that? For the first one, the one that I did a lot of research for the company size, let me think. About candlestar, including, yeah, the company size is very small and around 10 people, including CEO and the console. Okay. Yes. And for this company, you don't do research, right? I did research. Oh, we did research. Is that, that is the one you said you did a lot of research? Yeah, that one, the company called UClass. Okay. Yeah. And although there's also UX researchers, but as designer, you also did research, right? Yes. Okay. So we can focus on this, I guess, this, this experience? Yes, I think this is better. Okay. So maybe we can talk about this one. So yeah, I'm so I'm curious, like, what like as a designer, what is your process of like doing user interview or also analyzing user interview? I'm going to conduct it that. Yes. Let me think. So I remember at the time we collaborate with a product manager. And so before doing the interview, we had a lot of discussion, mostly mapping out the business problem and what is the\", metadata={'source': 'interview3.txt'}),\n",
       " Document(page_content=\"user problem as well. So all of those are based on their experience or what CEO realized and a lot of assumption. So we just clack a lot of idea or thoughts before we conduct the interview. And we map out some area that we want to learn more from users. And then we start to decide and discuss what it, what would be the user research goal? And also we designed the interview outline together. And then after that, we invited a lot of our existing users from our platform at the time. And a lot of them, they are really willing to participate in the interview. And we met them in person. So we met, I remember that it's only two weeks. And we met about 12 parents either in their house or in the coffee shop. And we met then and talk about our stuff. And there's always will be two people interview at the same time. So one person mostly asked the question and then another one, take the notes. And a lot of data. And we also record the voice. So we analyze the data afterwards, after our interview, how do you analyze the data? I remember because at the time, we are super new. And then we didn't quite know what is user research. But we will share what we realized the most the most stunning insights after the interview. And then we three of us mostly three of us, PN, me, and another UX researcher. He is also an intern, so two intern and one PN. And then we we will quickly discuss what we heard. And very cool surprising we're studying. And we will record that very quickly on our notes. And afterwards, we will listen to that together again. And while we are listening, we will drop down the keywords or highlights on the sticky notes. And then we clutter the sticky notes together and mostly the PN at the time, he will give them a title or a summarize for those for different groups. So that's how we did synthesize. Got it, got it. So what let's say when you're conducting this user interview and analyzing the user interview, do you find any process that is like time consuming or it's tedious for you? I think the most time consuming is while we, because we conduct the interview. And then we need to listen to it again. And then during the whole, like maybe one or two hours in the window to do that again and then drop down all the things that we have already heard, but we've got. And I think that is very long process. And after that, we sometimes, because we drop down notes individually. So we need to explain what we've dropped down if the other person don't understand. So listen it again. And I feel there's so many times we listen. I'm not sure if it's needed, but I sometimes I feel if you listen to the record multiple times, sometimes there's other thoughts over pop up. So maybe it's not. So maybe it's important, but I don't know, but I just feel we do a lot of things multiple times. And after we drop down the notes and then the PN needed to, because we kind of to junior and then we are unable to give the summarize or title to each clutter. And the PN spent a lot of\", metadata={'source': 'interview3.txt'}),\n",
       " Document(page_content=\"time, give them the title himself. And then remember that we wait by side and we just got him for like multiple hours. And yeah, I think it's the part. Yeah, that hopefully makes sense. Yeah, I think like you need to rewatch the video and you need to jot down the notes. And you also like the PN also need to spend time for like the affinity mapping, right, the cluster name. Yeah, it definitely like sounds takes a lot of time. Do you think like rewatching the video is like useful or like needed? Or what is the purpose for rewatching the video? Yes. So I think because while we were interviewing, we're trying to respect the interviewee and we want to look at the person. And then even there's a person, we all jot down notes, but it's very quick. And then we kind of worry that we mean something. So even we have a discussion afterward, we still want to revisit all the things and then think about it and then that the interviewer to jot down his notes or her notes. Yeah. So we're trying to capture all the things. So that is the purpose of why we watch. We capture all the things. Because I remember you mentioned like you think when you revisit the video, you can sometimes learn something more. Yes, right? Like I can't elaborate on that. Yeah. So what I would say this was sometimes I'm the interviewer and then why I'm asking questions. And then I try to focus a lot on the interview outline and asking questions and then listen to the person and then give them the reaction. So sometimes while they say something, I feel all this so cool, I want to shut down, but I can't. So while sometimes I will forget the information is kind of too quick to absorb and I sometimes miss something. So while I rewatch the video, we will watch it together. I was able to catch up something back that I missed. Yeah, that makes sense. Yeah. So when you conduct interview or record interview, do you use any tools? At the time, we are so like a very basic, we just use our phone. And sometimes we even didn't record a video, we only record audio, but we release it again. And we, oh, I remember at the time we have, we use a tool call something I forgot, but basically the function very simple. It just creates a mic map very easily, like a tree type of mic map. So we will have a person's name in the middle. And then what is the insights? And then we kind of jump there. So it's we will become a map of belong to that interview. Is that like AI generated? Or you need to do that by yourself? By ourselves. I think yeah, we didn't know what is AI. When was the internship? The internship was so many years ago. About five years ago. Yeah. It's all time ago. Yeah. Yeah, that's a long time ago. Well, but I think creating a mind map is such a good strategy though. Do you do that when you re listen to the interview or right after you interview with them? Just from your mind? That was why we are really listening. Oh, we're listening. Okay. Yeah. So sometimes we will use sticky notes. Sometimes we will\", metadata={'source': 'interview3.txt'}),\n",
       " Document(page_content=\"if we have a computer bias, we will do the mind map. But I think the sticky notes is better sometimes because we can move it or clotted it easily. But for the mind map, if you want to drag it or something, the structure will be break and it's kind of hard to organize it. So sometimes we will do the sticky notes and then organize all the insights into the mind map made. But I think the process is kind of flexible to us. So sometimes we just bounce back and yeah, definitely. And yeah, so after you conduct and analyze this interview, do you need to present to your stakeholders as a designer? At the time, mostly was the PM to present. And we, I think it's also a small present. So me as a product designer, the UX research intern, both of us kind of present the simply and casually to the product manager. And then he also will discuss with us because sometimes he can participate the interview. So then sometimes he can. So we kind of present to him and then we discuss. So it's kind of like a cat draw sharing. And then he will present to the our boss at the time. And when he presents, did he use any like visualization to make it look better or more clear? Yes. So he will create a very simple slide deck. And then he will make each of our points into like very simple and then I catch way, clear way. I think it's mostly like how we design the presentation. And sometimes he will put the users picture if the person feel comfortable to that the our CEO know that all this is from or some quotes with sometimes he will put the quotes. And then today he know that all this is really important to the person. And then this is the real insights is not from us is from our user. And yeah, he will. I think the rejoice kind of important while he was presenting. So he said he used the simple like no point. It's more like a bully point. And right. That's similar. Yeah. And then use like user picture with like their coats. Like is it that is that all or did he also use something? I don't like the affinity mapping or the cluster you guys did. He didn't put I I forgot. But he showed it to us is the very single summarize summarize. You do slide. And I don't think he put those clutter because so everything is so quick. And then I think for putting a clutter picture or something those details. Yeah, he think elbows won't care about that. He just want to know the results. So I think the slide's no simple. Yeah, got it. And yeah, I guess that is all the questions for the is current company that you worked that that company worked with. I guess like so in other companies do like as designer, have you ever also conducted research? Yes, for other company, mostly I conduct research in bigger company. I'm not sure if it's helpful for you, but my first official job in our design team is very small, but it's a big company. So I think it's I don't know if it's helpful for you, but at the time we also conduct survey and also use your interview. And the survey we also use some tool called\", metadata={'source': 'interview3.txt'}),\n",
       " Document(page_content=\"survey monkey because it will summarize for you. The they will give you the flow charts will give you the trend of how this survey was. And we also conduct interview, but the interview at the time we have another professional user researcher. So he will summarize all the thing and then create a deck to share also quotes and some insights to the whole company. So I didn't do much, but I do I did interview but that he to analyze. And I remember there's a tool called user testing. Yeah, so it has for user class. Yeah, we did we use that and I thought that is so helpful. So it will give you some highlights on the timeline. And then I think that might be a I like technology. Yes. And then my in my internship this summer. It's a it's also a bigger company, but we use a matter tool called Dovatore. Have you heard? Yeah, I heard. Oh, yes. Yeah. So that is kind of also help save a lot of time to summarize the primary research sites. And for my current company, we haven't do user research. We only look at data only. Okay. So for for the company that you did you you use a monkey. Do you know how big like you like around like the size that the company size about two to 300. Okay, two to 300. How about the company that you stop tell? I I oh you stop tell. Yeah, it's multiple thousand. I forgot. Oh, like like thousands of people. So it's like a large large company. Okay. And even in the large company as a designer used to conduct user interview. Yes. Oh, yeah, I'm curious like because I'm a UX researcher. And I feel like in my current company, most interviews are conducted by research, UX researchers. So why do you think sometimes UX designers also need to conduct interviews? Yeah, I think this is a really good question. I also have the similar question while I was interning this summer. So our design team structure is kind of special. Our team, we have multiple power designer and also two UX researcher, but they are mostly helping power designer to do the research. And then then they are they only do the research that directly impact the business stuff. So it's not a feature research. They do like higher level research. And then the features kind of a research power designer needed to do that. And we will then review our interview outline or our interview design by the research. But we conduct the interview. And I think it's quite good to me because I'm the one that going to design a feature. And I will not I will have some question that I want to know. I want to confirm with our user. And then while I'm doing the interview, I can dig down and I can ask the question I want to know. And sometimes P and we also participate interview. So it's kind of who designed this feature and the thing we'll kind of participate at to directly know the insights from the users. And also it's a power designer. Me, I need to synthesize and come out with the summarize summary and share with the whole team and come out with the feature together. So it's mostly power designer to\", metadata={'source': 'interview3.txt'}),\n",
       " Document(page_content=\"do that. Yeah, yeah, I think that's great. Like, so you so when you cause like the interview, is it before you design the feature or after you design the feature before before? So it's more like understanding your needs and your pain points, like, yeah, the deeper, yeah, that interview. Okay. And is that within the five like is it within the very large company? Okay. So you stop tell as well. Yes. Oh, okay. So when you use dovetail, do like do you find it like useful or do I mean, I mean, do you find any like pain points that you don't like? I feel that I think they are too part the part I like the most is you upload your video or something and then you will transcript all the things for you and you can easily like I like some parts and give them the tag or summary quick summary to the so I think the flow of this is easy for me because previously I needed to use Google sheets and I needed to color code on my own hand and that one that was helping me and I think that UX is cool, it's cool. But I think maybe I needed to have a more SOP for using that or have more rule because I can add a lot of tag and that the tag is everyone can add a tag. So there will be so many repetitive tags with the same meaning and it's not really helpful and it made me feel the tool is so complicated and even if the video is too complicated that will use more of my time and sometimes I would even want to just use Figgen or something and I think also the good part is because it's kind of the data depository so sometimes I can search keywords and I can watch other features interview but it's kind of related to my product to my project so I can learn more based on previous interview from other private signers. So I think the history is very helpful and also there's a area that really like Figgen it will make up all of your transcript into like a section if you highlight them so you can clutter those scripts without typing it down on yourself. I think that also saves a lot of my time and they will I think you also have some AI summary stuff. How do you have a new stuff? Yeah I do stuff but I'm a worry and I don't worry. I maybe it's because AI just start really quick and I haven't just this so I will worry that it will miss some insights that maybe the computer can't understand so I will still read all the things and then trying to figure out if there's something the AI summary means but I think it's a good start that I will still copy them and then make them into some bigger category but I will still do all the things again and then trying to see if there's something missing. So I think that is not really saving my time but I don't know about the better in the future maybe they should ask us all the risks. Is this cover all the insights you want to find or something to try to improve? I don't know. Yeah I think it was a really good point that we want to tackle. It's like we want to like just design something that make it really AI user friendly like make users to trust the\", metadata={'source': 'interview3.txt'}),\n",
       " Document(page_content=\"software so this just said maybe it could be better if it will ask the user to confirm like if the insights is enough or yeah to trying to build a feedback loop or something. Oh yeah I saw the same thing like because I think now the AI generator thing is just like directly give you like a summary but you cannot double check or like you cannot find evidence for that? Sure. Can you find what is the evidence you mentioned? Yeah the evidence for the summary Oh yes it just gives you all like some photo point but you don't know which one is come from where? Yeah you can't even sort back or sort you can sort back and it's some of the maybe some scripts you think it is so important but you don't know if it's included in each of the inside they provide. Yeah do you think it will be useful if if it asks you to compare like your own insights with the AI generated insights and yeah and then maybe you can add add your own insights after the AI insights or you can add it to the AI insights? Yeah I think that would be helpful. I think can add it it can be helpful and I think if the edits can be included in like a help the platform to improve their way to summarize I think that would be super helpful but I think I think you should that the platform should that the designer or researcher know about they are improving or what they miss or what they miss. Yeah just not just give you all the thing and then you still need to check. Yeah do you think it's better if you have like individual bullet points and each point can be linked back if you click on the bullet point you can link back to video and you can replay. Yeah I think it would be super cool if they give you a essay for one bullet point and I can click that to see how this AI to come up with this is from which scripts which video which section I think they will make me feel more that's worry because I can know if it missed anything or where it didn't come. Yeah all right that's that's all I want to ask and it's super helpful. Thank you thank you so much. Thank you. Do you have any questions like for for me or yeah or no? I'm just the platform mostly focusing on the primary research right? The the platform? Yeah yeah yeah. Yeah so we have almost one to focus on the primary research like the interview of user interview like well on one interview like why they want to do something what's their attitude to or something what's their pain point? Yeah but we are not aiming for the usability testing. Yeah okay okay. Okay I don't have any other questions. Thank you so helpful. I hope it's helpful. And you're really really helpful. Yeah okay. I stopped the recording.\", metadata={'source': 'interview3.txt'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c18f3968-150f-4967-917c-b6d81732cff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(texts, embeddings)\n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model=\"gpt-4-0125-preview\", temperature=0), chain_type=\"stuff\", \n",
    "                                 retriever=vectorstore.as_retriever(\n",
    "                                     ), \n",
    "                                 return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4343f26-641a-4136-b7e7-5830aba774df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "output = qa({\"query\":\"give me three insights in this video\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fc90a216-be3b-41d7-ac3e-216b1d2d19bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Based on the provided context, here are three insights from the discussion about the technology and user experience:\\n\\n1. **Human Element in AI Insights**: The conversation highlights a common concern with AI-driven analysis tools, such as Envision, which automatically generate insights from video content and transcriptions. While these tools can efficiently identify and highlight key points, there's a recognition that they lack the nuanced understanding and interpretive depth that a human researcher can provide. This underscores the ongoing need for human oversight in qualitative research to ensure that insights are not only relevant but deeply connected to the research objectives.\\n\\n2. **Affinity Mapping for Deeper Analysis**: The mention of an affinity map feature as a desirable tool indicates the importance of organizing and categorizing insights based on themes. This approach allows researchers to delve deeper into the data, uncovering patterns and connections that might not be immediately apparent. It suggests that while automated tools can assist in the initial stages of data analysis, the ability to manually manipulate and explore the data remains crucial for comprehensive understanding.\\n\\n3. **The Value of Non-Verbal Cues in Research**: The discussion about observing participants' interactions with scanning technology highlights the significance of non-verbal cues, such as body language and physical coordination, in user research. This insight emphasizes that understanding user experience goes beyond verbal feedback and requires attention to how users physically interact with technology. It points to the need for research tools that can capture and analyze both verbal and non-verbal user responses to provide a holistic view of the user experience.\\n\\nThese insights collectively underscore the balance between leveraging AI for efficiency and maintaining the depth and nuance that human analysis brings to research. They also highlight the evolving nature of user research tools and methodologies, emphasizing the importance of flexibility, human insight, and the ability to capture a wide range of user responses.\""
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['result']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "33fd85f9-442e-485d-8fc5-9ff6413024c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content=\"someone was launching that's kind of similar where it has the video, it pulls out the insights and then you can also go through the script, the text and highlight additional things that you think are interesting because I mean, AI does its best, but it doesn't have the human touch, right? I'll tell you the name of it. It was called, I was just looking in, it's called Envision, but it's spelled, are you familiar with it? I have a card of it. Is it, is it an V.I? I just, I copy the name and chat for you. So maybe it might be helpful to check it out since you're still in discovery. Yeah. And it was nice. It was organized. It had the video, it had the transcription already ready to go, but I think that, that human element for me was still missing. So I still prefer to go through and do it myself, maybe because I don't trust the insights that it's pulling as much as I trust my own brain. But I did like that it would automatically, once you uploaded the video, it would automatically come create the text file and highlight things that it thought were important to the research that would be useful for the researcher and for the team to see. Wow. And I know there's like other one, another one that somebody showed me, but I haven't used it yet. The problem is, is a lot of these are paid accounts. They're not free. So I don't really feel the need to spend the additional money. Maybe if the team grows and we can add something like that to the company repertoire, that would be nice. But yeah, I am trying to think of how like, I think having the affinity map feature would be nice as well, just because then you can, you can map all the insights by theme and play around with it in your own, because like maybe there's something a little bit deeper that you're trying to identify that's not on the surface, that you kind of have to look a little between the lines of what the speaker is, or what the participant is saying. Maybe there's like having the video is helpful because maybe what they're doing is also important. Like for example, my research study, when you, you take a body scan. So part of me, part of the research study is just seeing how people interact with the scanning technology. And if they're able to like, there's a different, there's a difference between people who may be more physically coordinated than not. So that's helpful for us as a team to identify where people are struggling and how we can make it easier and more user friendly. It's like a new piece of technology. Like a lot of people are really unaccustomed to having to like, do the pose and the spin and it's a little, some people, they get it right, right away and other people, they struggle a little bit. So it's interesting. Yeah, so it is not sometimes it's not all about the, what you said is also about how they react, how they move. I mean, during the user interview, what they click. Body language and that sort of thing. Like you can tell if somebody's getting frustrated, even if they\", metadata={'source': 'interview1.txt'}),\n",
       " Document(page_content=\"someone was launching that's kind of similar where it has the video, it pulls out the insights and then you can also go through the script, the text and highlight additional things that you think are interesting because I mean, AI does its best, but it doesn't have the human touch, right? I'll tell you the name of it. It was called, I was just looking in, it's called Envision, but it's spelled, are you familiar with it? I have a card of it. Is it, is it an V.I? I just, I copy the name and chat for you. So maybe it might be helpful to check it out since you're still in discovery. Yeah. And it was nice. It was organized. It had the video, it had the transcription already ready to go, but I think that, that human element for me was still missing. So I still prefer to go through and do it myself, maybe because I don't trust the insights that it's pulling as much as I trust my own brain. But I did like that it would automatically, once you uploaded the video, it would automatically come create the text file and highlight things that it thought were important to the research that would be useful for the researcher and for the team to see. Wow. And I know there's like other one, another one that somebody showed me, but I haven't used it yet. The problem is, is a lot of these are paid accounts. They're not free. So I don't really feel the need to spend the additional money. Maybe if the team grows and we can add something like that to the company repertoire, that would be nice. But yeah, I am trying to think of how like, I think having the affinity map feature would be nice as well, just because then you can, you can map all the insights by theme and play around with it in your own, because like maybe there's something a little bit deeper that you're trying to identify that's not on the surface, that you kind of have to look a little between the lines of what the speaker is, or what the participant is saying. Maybe there's like having the video is helpful because maybe what they're doing is also important. Like for example, my research study, when you, you take a body scan. So part of me, part of the research study is just seeing how people interact with the scanning technology. And if they're able to like, there's a different, there's a difference between people who may be more physically coordinated than not. So that's helpful for us as a team to identify where people are struggling and how we can make it easier and more user friendly. It's like a new piece of technology. Like a lot of people are really unaccustomed to having to like, do the pose and the spin and it's a little, some people, they get it right, right away and other people, they struggle a little bit. So it's interesting. Yeah, so it is not sometimes it's not all about the, what you said is also about how they react, how they move. I mean, during the user interview, what they click. Body language and that sort of thing. Like you can tell if somebody's getting frustrated, even if they\", metadata={'source': 'interview1.txt'}),\n",
       " Document(page_content=\"it has a lot of like, this would be the best avatar\\nanyone's ever made in 2K. Like no one's ever done a 2K face scan and had it look this good, but it's still not as\\ngood as a perfect reality. It's a, you've heard the\\nuncanny valley thing before. I think the number one\\nweakness for the avatars or the Personas that I've seen is hair. So basically everyone I've talked to has like a frozen lump of hair instead of flowing realistic hair. And that's true about all flowing things, like however your hair\\nwas when you did the scan, it's frozen that way. And so is any necklace you're wearing, whether it's crooked or not, or I guess, technically\\nalso any makeup you had on, or however you looked\\nwhen you did the scan. Maybe that could be a good thing. Maybe you did a scan when you\\nwere looking all dolled up, and then you get on a 7:00 AM call, and you still look perfect even though you look like you just\\nwoke up in real life. So I guess there's that too. But anyway, all that is to say FaceTime. FaceTime is the most well thought out, like most futuristic\\nVision Pro experience. It just is. So I'll end this video with this. Now you know what it's\\nlike to use and operate the Vision Pro. But there's still a lot more to consider when actually considering\\nif you should buy and own this thing, from the use cases, to the things that work\\nwell, and don't work well, the philosophy behind it, the\\nprices, all of that stuff. That's what's gonna be for my full review. Like there are parts of this thing that are absolutely amazing, unparalleled, best I've ever seen. But the reason it's so interesting is because it's actually a young category. Like we're so used to this\\nslow, boring iteration in mature categories, like\\nsmartphones, and laptops, and you always see the comments talking about how tech is so boring, but now they're actually\\njumping into something risky, and it's actually fun, and\\nthere is downfalls and flaws, and it's fun to actually\\nweigh the pros and cons. So I'll be expanding on all these way more in the full review, but\\nI'll leave you with this. I've got my upsides and\\ndownsides to Vision Pro. It's been a week. Upsides, some of the stuff\\nthat's the best I've ever seen in a headset. Immersion, placement\\nin space, eye tracking and hand control, passthrough, ecosystem, and spatial audio. And the downsides, weight and comfort, the eyes on the outside, app selection right now,\\nbattery life, and price. So the full reviews in the works. Definitely get subscribed\\nto be among the first to see that when it drops. Either way, till the next one. Thanks for watching. Catch you later. Peace.\", metadata={'source': 'transcripts/7bde9362-4aed-40ad-a505-8410f8d3449a/dtp6b76pMak.txt'}),\n",
       " Document(page_content=\"they're gonna work with it. It's interesting. I really feel like the biggest\\nstrength of the Cybertruck is it's gotta be the\\npower, the sheer force of 2.7 seconds, zero to\\n60, 850 ish horsepower like no other truck moves in a straight line like this one does. But I'm also gonna include\\nthe steering as a pro, you gotta get used to it. But again, that tight turning radius and how nimble it feels like on a highway that's remarkably responsive. Like I can turn my wheel\\nlike this in a Model S Plaid and it does not feel\\nas responsive as this. That's a crazy thing to say. So then it's downside is gonna be one, just that people think\\nit's ugly and that's fine. A lot of people are gonna think it's ugly. And two is it's more expensive\\nthan we originally thought. It doesn't have that price advantage. I am driving a hundred thousand\\ndollars truck right now. There's no way around that. The F-150 Lightning\\nI'm following right now is gonna cost significantly less. The Rivian R1T and the and the Hummer are also very expensive, but it's no longer an advantage that we might have\\nthought it was gonna be. Now this isn't a review. I haven't towed anything with this. I haven't used any of\\nthe trailer features. I haven't filled it up with rocks and mud and gotten it dirty. I haven't off roaded at all. There's a total separate off-road mode in the thing I haven't used. I haven't road tripped in it. I haven't lived with it. I do have an order so I\\nplan on doing all that. So make sure you get subscribed to see the full review of the\\nCybertruck when it comes out. But I think generally, as this\\nis my second impressions ever and first time ever driving the thing, I think they really got\\nthemselves something solid here. We're gonna have to see\\nabout fit and finish. A lot of, let's see,\\na lot of wait and see, but I'm impressed with this. I'm impressed. So lemme know what you think. Obviously, there're gonna be some polarizing thoughts on the looks. Let me know if you're in the, this thing is ugly and\\nI'll never buy it camp. Why is this thing looking like a stainless steel refrigerator on wheels? I totally get it, but lemme know what you\\nthink about Cybertruck versus F-150 Lightning versus Rivian R1T. Let me know if there are other things you want to see in a video then get subscribed to me among the first to see that stuff. I think that's about it for this one. Thanks for watching. Catch you later, peace. (upbeat music)\", metadata={'source': 'transcripts/7bde9362-4aed-40ad-a505-8410f8d3449a/XxOh12Uhg08.txt'})]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['source_documents']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e8a7a0-b9bc-48a0-99ca-b30a8f50f884",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8856d72d-4af4-4e10-841b-18db16cf00d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "f287c1bb-7500-4a41-be69-0223a01624fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The videos are about using the Vision Pro.'"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca47f9e-677c-4e87-8ec1-6e2461621b7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "04c26091-4f30-4ffb-80bb-daebac1747ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "b880af69-f346-4486-a302-75e344429d5a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from pathlib import Path\n",
    "from pprint import pprint\n",
    "\n",
    "def save_to_json(transcript,filename):\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(transcript, f, indent=4) \n",
    "\n",
    "def load_documents(filepath):\n",
    "    def metadata_func(record: dict, metadata: dict) -> dict:\n",
    "        metadata[\"start\"] = record.get(\"start\")\n",
    "        if \"end\" in record.keys():\n",
    "            metadata[\"end\"] = record.get(\"end\")\n",
    "        return metadata\n",
    "\n",
    "    loader = JSONLoader(\n",
    "        file_path=filepath,\n",
    "        jq_schema='.[]',\n",
    "        text_content=True,\n",
    "        content_key=\"text\",\n",
    "        metadata_func = metadata_func\n",
    "    )\n",
    "\n",
    "    documents = loader.load()\n",
    "\n",
    "    return documents\n",
    "\n",
    "transcript = YouTubeTranscriptApi.get_transcript(\"dtp6b76pMak\")\n",
    "save_to_json(transcript,\"transcript.json\")\n",
    "documents = load_documents(\"transcript.json\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "eb904a0d-a319-421c-94fa-28af2c5c4dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: model not found. Using cl100k_base encoding.\n"
     ]
    }
   ],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "vectorstore = Chroma.from_documents(documents, embeddings)\n",
    "qa = RetrievalQA.from_chain_type(llm=ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0), chain_type=\"stuff\", \n",
    "                                 retriever=vectordb.as_retriever(\n",
    "                                     # search_type=\"similarity_score_threshold\",\n",
    "                                     # search_kwargs={'k':5, 'fetch_k': 50, 'score_threshold': 0.7}\n",
    "                                 ), \n",
    "                                 return_source_documents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "909545d5-0c25-4d4d-9dda-907915baccdd",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(upbeat music) - All right, so you've seen the unboxing. Now it's time for the breakdown. What is using the Apple\\nVision Pro actually like? This is easily one of Apple's\\ncraziest, most radical, possibly dystopian products of all time. And I have a lot of thoughts here, like I've been using it\\nfor about a week now. There are some parts of this thing that are absolutely incredible, and some other parts that feel weird, or borderline unfinished. There are all kinds of new technologies, from a new operating system\\nto infrared eye tracking to virtually reconstructed\\nversions of you. I feel like there are so\\nmany actually new things that you have to understand\\nin order to get a sense of what this headset\\nactually is and what it does. So I'm gonna break this\\ndown into two parts. This video is all about\\nusing the Vision Pro. It's everything I've\\nlearned from the past week of wearing and getting used to\\nthis thing every single day. But I'm also working\\non a more wide ranging, possibly more existential, review video. But let's just start with the more hardware fundamentals, right? Like what is this thing\\nthat I'm holding literally? Apple Vision Pro at its core, well, it is a VR headset. Now, Apple would never say that, and they probably won't like\\nthat I'm saying that word. You know, I made an entire video about why they refuse to use those words, and they're calling it\\nspatial computing instead. We'll get there. But the truth is it's a\\nreally, really, really high end virtual reality headset. It's something we've seen before, right? It's got displays and lenses and speakers and fans and buttons. And this is a form factor. This is a thing that we have seen before, but before I even turn this thing on, there are clearly several things that are a little\\ndifferent about this one. So first of all, it's made of metal. Lots of metal and glass\\nhere, which are high quality, but heavy materials, relatively speaking. So there's this precisely\\nmachined aluminum frame around the outside. And yes, those are intakes\\nfor fans at the bottom. And then vents for those fans at the top. On the right side,\\nthere's your digital crown that can be pressed in or turned. And then on the other side is\\njust a single larger button. So kind of basically the same\\ntwo buttons as an Apple Watch. And then when you get\\na little further back on this band here, these little pods with downward facing\\ngrills, these are speakers which are pointed straight at your ears, and work surprisingly well. Though of course, it also\\nmeans that people around you can hear a little bit\\nof what you're hearing. There's a little bit of bleed, and I have a lot to say\\nabout spatial audio, so stay tuned for that. But the main event is at the front. There is an enormous piece of glass, which, yes, is very easy\\nto fingerprint and smudge. And then behind that thing,\\nthere's this outward-facing OLED display and a bunch of\\nsensors all the way around, outside facing sensors that go forward, sideways, and straight down. And there's depth sensors,\\ninfrared illuminators, lidar scanners, and just\\nregular old RGB cameras, all being processed by an M2 chip and an R1 chip inside this thing. And then maybe the craziest part, inside the headset, there\\nare a bunch more sensors facing your eyes, tracking\\nyour eyes in real time, for all the eye control and\\neverything that comes with that. And also then to display a\\nrepresentation of your eyes on the outside of the headset. Kinda, we'll get there. But overall, when you put it all together, you get a very well made, very high end, but also pretty heavy\\ncomputer to wear on your face. So officially, this headset\\nwith this solo knit band when I weighed it, showed up as 638 grams, which some of you on Twitter\\nhave already pointed out is actually slightly less than\\nthe plastic Meta Quest Pro. But that Quest Pro also\\nhas a lot of battery on the back of your head as\\na sort of a counterbalance, so the weight distribution\\nis very different. Also, the Quest Pro is not\\nthat comfortable anyway. But the point is this,\\nfor Apple, made the choice of taking the battery off of\\nthe headset, which means okay, now there's nothing on\\nthe back of your head, so you can wear it and\\nlean up against things, and that might be an upside, but that also now means you have to deal with this cable all the time\\nrunning up to your head, and the fact that it's\\nvery front weighted now. All of the weight is on\\nthe front of your face. So this is the battery, as\\nyou saw in the unboxing. If you haven't already seen the\\nunboxing, that just went up. I'll link it below the like button. But this battery is a surprisingly small 3,366 milliamp hours. I say surprisingly small\\nbecause a normal battery bank of this size, you might\\nexpect to be 10, 15, 20,000 milliamp hours. I suspect there's a lot of\\nheat insulation happening here. But it comes with a\\nnon-removable four foot cable, and a proprietary connector\\nat the end of the cable that will twist and lock to the headset. And so the lock is really solid. It makes sense that it's\\nnot just straight USB that could get disconnected easily. Once you connect it, it starts glowing, and then it starts booting up. And there's even a little\\nApple logo that displays on the outside screen\\nwhile it takes, you know, a little under a minute to turn on. So there is no on or off button or switch anywhere on this headset. Maybe kind of like AirPods\\nMax or something like that. So if you ever take the headset off and put it down, it will\\nenter a standby mode after some time, but it won't turn off. If you wanna turn it off,\\nyou literally have to twist and unplug the cable. That's the only way to\\nactually turn the headset off. Now famously already, the battery life with this included\\nbattery, is not super long on this headset. Two to four hours is actually realistic for what you can expect for\\njust like this built-in battery. But that's also kind of right in line with a lot of other VR headsets. Battery life on VR headsets\\nis not that great in general. If you do wanna use it longer,\\nthe only way to do that is there's USBC port on the battery, and you have to plug the battery in. So you could plug the\\nbattery into the wall for infinite battery life, or I guess you could plug it into like a, you could daisy chain another\\nbattery into the other pocket or something for even longer life. But yeah, two to four hours. Now at first it seemed weird to me that the port is on the\\nsame side of the battery as the non-removable cable, but I think it's because\\nthey just want you to default to putting this battery in your pocket, probably in your back pocket. So even if it's plugged into the wall, it can still be in your back pocket. You're just gonna want to\\nget a longer USBC cable. So there are no controllers\\nthat come with this headset. Now it does support other input methods that are like game controllers,\\nand mouse, and keyboard, and those can be incredibly useful, but by default the primary input method for everyone using the Vision Pro is your eyes and your hands. So the first time you put on this headset, it goes through this calibration process, and it's pretty interesting. So the first time you ever put it on, it first adjusts the\\ndistance between the lenses, physically moving them inside the headset to match the distance between your eyes. Then it does this sort of a hand scan so it understands your hands. And then you go through this process of basically looking at a bunch of dots all the way around the screen, and then tapping your fingers\\ntogether to select them. Kind of feels like an\\neye test or something. And then you're in. So first thing you're gonna notice is you can actually kind\\nof put your hands anywhere as long as the headset can see this, just your fingers touching together. So there's a lot of pictures\\nof people using a headset with their fingers, like\\nout in front of them, pinching like that. But you actually don't have to do that. It's such a wide angle\\nbecause of the sensors facing forward and sideways and down. You can kind of just\\nrest your hand anywhere, in front of you, in your lap. As long as you pinch like that, it can generally pick it\\nup, which is impressive. So you're pinching to control anywhere in that 180 degree bubble in front of you. And then the digital\\ncrown, you hit that once, and the app drawer\\ncomes up, pretty simple. Doesn't seem that impressive. But this is actually a peek\\nat the first really impressive thing about this headset to\\nme, which is it seems to have incredible spatial positioning lock, and like, it's really hard\\nto have you appreciate this through a YouTube video. Reviewing VR headsets is hard. But turn around in the room you're in, and picture a wall or a window just appearing locked in place\\nin 3D space in your room, and no matter how much you move your head, or move around, it stays\\nexactly kind of floating where it's supposed to be. But when I say floating, I\\nthink you're picturing like a, a soft float, but it's locked,\\nand that's how it starts. So now you're in Apple's new Vision OS I would describe this as\\nkind of similar to iPad OS, but way more glassy, and of course with the extra dimension of 3D space. So hitting a digital crown\\nwill always get the app drawer back in front of you, and\\nthen simply look at the icon you want and pinch your fingers together, to select it and open that app. Scrolling is basically as you'd expect, you just kind of pinch\\nand grab in the air, and then pull as if it's on a string, and physics let you pull\\nthings through the air. It's pretty intuitive, it's\\nresponsive, it's fluid. Sometimes it's kind of bouncy even. I would say the biggest adjustment is only being able to control exactly what you're looking at. And I don't think people realize how often they're controlling things that they're not exactly\\nlooking directly at with other computers and other UIs. But with this, you can look\\nat the button to select it, and if you look at the\\nnext thing you're gonna do, you're no longer controlling the button. You have to look exactly\\nwhere you're trying to interact with things. It takes a few extra brain cycles to remember to always be looking exactly at the thing you're controlling. So when you open a window\\nof a Vision OS app, like any one of the\\ndefault Apple apps here, it locks into place, it's floating there. It kind of looks, again, like an iPad app, but very glassy, like this\\nfrosted glass around the UI sort of lets you see through a little bit to the color behind it. And it even sometimes casts\\na shadow on the ground in the correct Z space,\\nso it really solidifies that it's floating in front of you. All this makes it feel like the window is in the space around you. Then if you look at the\\nbottom of the window, you get a little bar, you can\\nalways just look at that bar and pinch to drag it around. So drag it forward,\\nbackward, anywhere you want in X, Y, and Z space, and then let go and it just stays absolutely locked. And then you can look at\\neither bottom corner to resize to make it bigger or smaller. And then finally there's\\na little X at the bottom, you select that, that closes it. So that is the basics of Vision OS, and just using an app. Now this entire time, by default, and almost any time they\\ncan, passthrough is on, which means you have the headset on, but you can see with the cameras right through to everything around you. And I think this is\\nwhere Apple really wants to normalize the term spatial computing, because it feels like augmented reality. It feels like you're always\\nable to see the space around you, but technically\\nit's not actually AR, because you are still looking\\nat a reconstructed version through a camera feed\\nof the world around you instead of the actual world around you. But maybe it's all just semantics. I will say, this is the best\\npassthrough of any VR headset I've ever used, and it's\\nnot even that close. Now again, it's so hard to get\\nthis through a YouTube video. It does have screen recording built in, so I'm gonna try to use that. But imagine putting a headset on, and not really feeling like\\nyou're looking at a screen with the real world. Because of the pixel density, because of the 90 hertz refresh rate, and because of the impressive\\ndynamic range of the cameras and the correctly adjusting shutter speed, you just almost don't, you almost just feel like you're looking at the real world, not through a headset. Also the passthrough is\\nso close to real time that I could legitimately\\ninteract with all kinds of things. I could catch items flying at me. I even tried playing ping pong. It was easy, no hesitation. So officially, the R1 chip\\nis doing all the processing of all this stuff and\\nadjusting the shutter speed for different lighting conditions and always keeping passthrough latency under 12 milliseconds, which\\nis the lowest in the industry. But it's really combining\\nthat with how close to reality the colors and brightness\\nand everything are that keeps it feeling kind of real. Basically, the only noticeable restriction is super close up items and\\nobjects can get a bit blurry, and then you can't quite\\nmake out really small or fine texts, so you can't read an email or a tiny text on your phone in your hand, but you can absolutely text people, or read your notifications,\\nwhile keeping the headset on. If you've tried other VR headsets, you know how impressive that is. It's just, it's really good with the tech that exists now for VR headsets. But you can definitely\\nstill take the headset off and be like, oh, it's way brighter in here than I thought it was. Either way, that's all passthrough, but if you ever wanna\\nfully immerse yourself, I mean it is a VR headset after all, all you gotta do is rotate\\nthis digital crown clockwise, just keep turning it, and it will slowly dial your environment more and\\nmore into your field of view until you dial it all the way\\nup to fully surrounding you. So all of the windows\\nyou might have had open will still stay stuck where they were, but everything you're doing\\nis just on the moon now. So yeah, there's a couple environments Apple has built in here, most of them relaxing scenic locations, like in California somewhere, or one really nice one is Mount Hood with a little bit of rain falling. They're not quite photorealistic, but they're just short of photorealistic, like they're the most\\nrealistic digital environments that I've seen. So then the last two big quirks\\nof the UI, control center. So the only way to get to control center is to look up, and you can't just look up, but you have to physically\\nturn your head up and look at this arrow\\nthat appears above you. So once you see that, you select that and then you get your control\\ncenter for things like, you know, battery life and\\nnotifications, focus modes, and screen recording,\\nand pairing to a Mac. But the other big quirk is text input. So you might be wondering\\nhow does text input work with no physical controllers? So there's basically\\nthree ways to do this. So let's say you are in Safari, and you want to go to mkbhd.com. You really want one of those\\nshiny new Chevron hoodies for the rest of winter. Great, how do you do it? So the first way is to\\nliterally hunt and peck poking the keys on the keyboard that appears in the air in front of you. So this one is tough, because\\nit literally only reacts to your pointer finger on each hand. So you actually can't type fast, like with home row or anything like that. Not great. The second way, though, I\\nthink is actually kind of good. It's at least faster,\\nwhich is looking at the key you want to interact with, and\\nthen pinching to select it. So just looking around\\nthe keyboard like this, and selecting the keys. And you might be surprised how\\nfast you can type like this if you actually know your way\\naround a keyboard pretty well. I actually prefer this to\\npoking the virtual keys because I at least get a\\nlittle bit of haptic feedback from my own fingers tapping together. But then in Safari, the last way to do it is literally to just\\nlook up at the microphone and say the URL out loud. MKBHD.com. And then it just hears you and goes to the site pretty quick, if it's a URL that you\\ncan actually say out loud. So, what can you actually\\ndo with this thing? Like now that we know what\\nit is, it's the M2 chip, a computer on your face with the displays and the lenses inside, and all\\nsorts of sensors everywhere. What can this thing actually do? And I feel like the most\\ncommon way to phrase that is what is the killer app? Because that's, we feel\\nlike we need some sort of justification to spend\\nthree, $4,000 on this thing. Like applications made the\\niPhone what it is as we know it, like apps made the iPad. So what is the app\\nsituation on the Vision Pro? So there are two types of apps\\non the Vision Pro, actually. The first is apps that\\nare built specifically for the Vision Pro to take advantage of its awesome experiences. And there are a few of those right now, and then there are all the other apps, which basically are iPhone and iPad apps that happen to be compatible because the developer didn't opt out. And the first kind is way cooler. So these are Apple's stock apps here that come with the Vision Pro. And so these are all, of course,\\nmade just for Vision Pro. So they're gonna have stuff\\nthat takes full advantage of what this thing is capable of. Apple Music is a pretty classic one, like it has all the same functionality of any other Apple Music app, but in this super glassy frosted window, and shows the colors of\\nwhatever's behind it. And you have the sort of sorting\\nmenu on the left hand side instead of across the bottom. That's the basic layout. Same thing with the Notes\\napp and the Settings app. Very glassy, almost looking\\nlike an iPad app in the air, just rebuilt with this\\nnew material design. And then there's the media apps. So Apple TV and Disney+,\\nthey both come pre-installed, which they have built entire environments inside of them for watching media. And there's even a small\\ncollection of videos on the Apple TV app that are shot on a new proprietary format specifically for Vision Pro. So it drops you into a space\\nwith a full 180 degree video, and Alicia Keys walks right up to you and starts singing right to your face. It's crazy. There's also the Photos app, which will let you look\\nat panoramic photos, for example, in this fully immersive view. So you can blow them up to full screen, and then it gives you a\\nbit of a parallax effect around the edges, so it\\nfeels like you're looking into a window of your own\\nphoto and looking around. It's kind of incredible. And then there's also\\nsome other really fun third party apps that I've tried that were built ahead of time. So Sky Guide, this is a good one. You can look around a real\\nrepresentation of the sky around you or any of the\\nconstellations would normally be, you can look at it a little\\nlonger and it'll pop it out. You can pull it outta the sky to get more information about it. It's a pretty great idea. There's another one called Jig Space, which is, it's a sick app, I don't know if I'd ever use it, but basically it lets you load 3D models into the space you're in\\nand mess around with them, take 'em apart, view them in actual size. And this really takes\\nadvantage of how good the placement lock is on the Vision Pro. And you can walk around,\\nand really gets you a better understanding\\nof the scale of things that you don't get to\\nsee up close very often. And then Keynote is another funny one. So you can of course go\\nthrough and edit a Keynote just like normal if you want to, but then they've built\\nthis whole environment for practicing your presentation skills. So you press that and it\\nsays, oh, would you like to go to a conference room, or the\\nliteral Steve Jobs Theater, so you can rehearse\\ntalking to your audience with your Keynote slides behind you. It is genuinely incredibly immersive. And there's already a\\nbunch more apps like this in the App Store already at launch that are specifically\\nbuilt for Vision Pro. So they'll take advantage\\nof its various strengths. Now, are any of these a killer app? Not really. I mean I don't, if you're\\nlooking for any one of these to be the reason why you spend\\nlike $4,000 on this headset, I don't think we have that yet. But then at least there's\\nall the other non-native, but technically still compatible, apps that are in the App Store. And these are gonna look just\\nlike iPhone and iPad apps. Actually, there's a pre-installed\\nfolder on the home screen when you get this thing\\nliterally called Compatible Apps, and there's a bunch of\\nthem from Apple here. They look exactly like iPad apps. I'm surprised actually\\nthat more of them aren't fully built out to take\\nadvantage of Vision Pro, but like, Apple Maps is just the iPad app. And so it would be cool\\nif there were some fun augmented reality overlay\\nwalking directions type stuff, but nope, it's all the\\nexact same functionality that you would find if you\\nopened this app on your iPad. And you can go to the App Store and search a bunch of the names of apps you already know and love,\\nand find them by name and grab them, and they'll\\nwork the exact same way. Crazily enough though, there are already some notable exceptions. No Netflix app for the\\nVision Pro, no YouTube app for the Vision Pro, no Spotify\\napp for the Vision Pro. Apple has kind of a\\ncontentious relationship with a lot of developers right now, especially some of the bigger ones. And so some have made the\\nactive choice to opt out. They're like, we don't wanna be there. This won't be a big enough\\nplatform to matter to us to justify the work. So they're not there. Now I totally get it, but also now as a Vision Pro owner and someone who's using it, I'm like, oh, it's kind of a bummer. I really wanted to be able to\\nwatch a Netflix show offline, downloaded it ahead of time,\\nbut you can't do that now. But at least, at least\\nfor now, for the record, you can use the browser, and anything that would\\nwork in the browser. So if you pull up Safari, and you get a full screen\\n4K YouTube video going, and locked in space, or\\neven in an environment, it looks great. It's razor sharp. Like, I could totally watch\\nYouTube videos like this. But you will definitely\\nbe missing the features of having the dedicated\\napp, like offline video. Honestly to me, the killer\\napp of the Vision Pro isn't just an app, it's\\nactually the ecosystem. And we knew this was coming, but the second you log into a\\nVision Pro with your Apple ID, immediately it starts\\npulling all the services, and all the stuff that you're used to from all the other Apple\\ndevices you already have. And I said this before the\\nVision Pro was announced, I was like, this is the most\\nobvious strategy for Apple because there are lots of people out there who have never considered\\nbuying a VR headset that are considering only this one because they have an iPhone, and this is the one that\\nworks with the iPhone, and none of the others\\nare particularly close. So all of your iMessages are already here, all of your photos are already here and loaded up and backed up. All your Notes are already\\nat your fingertips. You already saw the Keynote app. But okay, easily my favorite feature is connecting to your Mac, right? So anytime your Mac is in front of you and it's turned on, hit that arrow and then there's this little icon to Become My Mac's Virtual Display. So I click that, and then pick my Mac, and it pretty much instantly,\\nit actually blacks out the display of my Mac, and\\nthen turns that display into a 4K window inside of the headset. So now my keyboard and\\ntrackpad still work, even if it is a desktop. The keyboard and the trackpad\\nstill control everything, and you can continue using it\\njust like a normal computer, but with the ability to\\nmake your new 4K monitor as big or as small or close\\nor far away as you want, which is super sick. And then the bonus is\\nyou can still open up and place other Vision Pro\\napps around your Mac computer. So like you can have your\\nMac in the middle here, and maybe you're editing or\\ndoing some work on the Mac app, and then you have a Safari\\nwindow, or Messages, or whatever else you want\\nright next to it around it. And then your keyboard\\nand trackpad can move seamlessly between them\\nall to control all of them. This, to me, as a Mac user,\\nthe ease of use for setup to make this happen, this feels like the biggest game changer, like the most compelling\\nfuturistic feeling use of this headset to me. Especially on a plane. Oh my god, I can't tell you how many times I've had an awkward conversation because, like, I'm editing a video on the plane, the person next to me sees\\nI'm editing a video of myself, and it's kind of weird\\nand hard to explain, but I'm picturing putting the headset on, the display blacks out, but now I can do all the editing I want, and I can make the\\nscreen as big as I want. So I've really enjoyed using that feature. Again, the biggest challenge, though, is still remembering to\\nlook exactly at the thing you want to control. So aside from typing on the real keyboard on whatever window is open, if you want to control something, you\\nhave to be looking at it. Again, it doesn't sound like a big deal, but when you try it,\\nyou'll see what I mean. And then also, odd\\nlimitation, one monitor only. From the Mac, one virtual\\nmonitor only at a time. So if you usually run a\\ndual display setup like I do for Final Cut Pro, big\\npreview on one side, timeline on the other\\nside, you can't do that. You have to use the big one\\nmonitor version of your setup. All right, so you might have realized I've left one thing out this whole time. One thing, you could call\\nit one more thing, sure. It's one more huge crazy thing, but it's kind of the defining\\ncharacteristic of this product and that is Personas. So in all the advertising\\nyou've seen of Vision Pro, there's these eyes on the\\noutside of the headset that looks like they're\\nkind of in a passthrough, like in a dark astronaut\\nhelmet type of thing. Easily the most memed, most unique aspect of this headset, right? It's the only headset\\nwith an outward display. And I mean it's very, very\\nprominent in those videos, but in real life, as you've started to see from some of my footage,\\nit is very different, and I think I figured out why. So first of all, it's not\\nactually see-through, right? There's a whole bunch of computer in between me and you right now. So the eyes aren't on the outside. It's a representation of my eyes based on what all the sensors\\non the inside are seeing. It's reconstructing it on the outside. So those sensors are tracking\\nat 90 frames per second, and they give you optic ID, which is, it's how you log into the\\nheadset and keep things secure. It's basically the same\\nas face ID, or touch ID, it's just looking at and\\nidentifying your eyes. And it also powers the one\\nbeta feature of this headset, which is Personas, which\\nis, it's the most impressive and weirdest thing about this\\nheadset at the same time. I'm calling it right now. So the purpose of the eyes on the outside is really not for you,\\nthe wearer of the headset. In fact, you'll never see it. But it's for the people around you. So when you're in a passthrough mode, your eyes will shine through to indicate that you wearing the headset can see the person outside. So that right there is\\nalready pretty unique. But then, when you're\\nin something immersive and you can't see what's around\\nyou, it covers up your eyes with this sort of like a blue,\\npurple glowing animation. So that intuitively makes sense. You can see the eyes\\nwhen they can see you, you can't see the eyes\\nwhen they can't see you. But crazily enough, there's also a feature where if you have someone\\nwho's outside the headset looking at you, talking to you, and you are in an immersion, but you want to talk to them through that, they will kind of appear through the fog of whatever immersive\\nenvironment you're in. So you just start talking and\\nlooking in their direction. It detects that, and sort of\\nparts a little bit of a fog and that person's eyes\\nwill show through the fog. It's pretty decent. It basically only shows\\none person at a time. And when this is happening on\\nthe outside of the headset, it shows a little bit of\\nyour eyes poking through the purple and blue glow. It's, as you can see, it's all working, but also, I think it looks\\nnothing like the eyes from the ad. So in an effort to make the eyes as presentable as possible, two things. First of all, this\\nscreen is actually behind a lenticular film, which\\nI didn't even realize that from the initial media they had published. But if you've ever heard of that, it's sort of what gives it this 3D depth. You might have seen this on\\nother holographic displays and stuff, but the point of that is to make the eyes appear to\\nbe sunken into the display, like on your actual face, instead of glued to the\\nfront of the headset, which would look a little more weird. But then two, to represent\\nyour actual eyes, they've built in a way to scan in and create a digital\\nrepresentation of your face, which is called your Persona. And it looks like this. So to get those eyes on the outside of the Vision Pro headset,\\nyou have to do something called registering your Persona. This is how it creates\\nthe digital version of you that includes your eyes\\nthat will show up here. So let's do that now. It's actually kind of a cool process. So I'm gonna put it on, and\\nhopefully the screen recording works so you can see\\nexactly what I'm doing. I'll hit the digital crown. I'm gonna go to Settings. And you can do this when\\nyou first set it up. But I'm going to Persona, and\\nI'm gonna hit Get Started. So let's refine my hands real quick. This is capturing detail\\nfrom the front of the headset of the hands in front of me. Once it's done with that- - [Automated Voice] Your\\nPersona, remove Apple Vision Pro. - It's gonna ask me to take it off. So this is how it goes. - [Automated Voice] When you're ready, hold Apple Vision Pro at eye level. Keep your arms and shoulders relaxed. Align your entire face within the frame. - [Marques] My face shows up like face ID. - [Automated Voice] Slowly\\nturn your head to the right. Now slowly turn your head to the left. Now tilt your head up, then tilt your head down. Next, let's capture\\nyour facial expressions. Smile with your mouth closed. Then make a big smile\\nwith your teeth showing. Now raise your eyebrows. Close your eyes for a moment. Capture complete. Put Vision Pro back on to continue. - I will do that. So now I have a menu that\\nsays Creating Persona, and it says it's in beta, and now there's my Persona right there. Kind of uncanny. The hair's a little bit\\ndifferent, but the face. Wow, wow. Okay. So there's different lighting. You can choose it to always\\nbe in studio lighting, and always be in contour lighting. I'll just leave it at\\nnatural, and hit next. You can change the color\\ntemperature of your skin tone. Cool to warm, I think I'm around there. Brightness, darkness. I think I'm around there, near the middle. Next. And then I can add glasses. So if I typically have glasses, which obviously I wouldn't be able to wear in the Vision Pro, you can still\\nlook like you have glasses, anytime you're on that FaceTime call. And then next. Save. And that's it. So I think now you should see my eyes. Maybe. And that that's the\\nthing, it barely shows up. You can barely see my eyes\\nwhen I'm wearing the headset. Now I've tried a couple\\nother scans subsequently, so I've tried different\\nlighting conditions, I've tried different\\nbackgrounds, simple backgrounds, tried different shirts\\nand things like that. It doesn't really ever\\nappear any brighter. I think if you have a\\ndarker skin tone like me, just don't expect the eyes\\nto show up very brightly on the outside of the headset. It's pretty subtle. Even when it does show up,\\nit's a little weird looking. The eyes are a little\\ntoo far apart sometimes. They're a little dim. You see one eye at a time. It's kind of weird. But that Persona though. Whew. That is some pretty interesting stuff. It's crazy that this is actually\\na real thing being shipped, like first Meta started doing it. Now Apple's doing this. This is, again, it's technically in beta. So I dunno, there's room for improvement, but it still works. But as of right now, I feel like this is both incredibly impressive\\nand slightly unsettling. Like, it's very impressive\\nthat this thing, this headset I'm wearing on my face, is tracking all these\\nlittle micro expressions and little movements for\\nmy eyes and my cheeks and my mouth and everything. But at the same time,\\nit's just not quite human. It's right at the edge\\nof the uncanny valley of I'm not looking at a person. So yeah. But the crazy part is you\\ncan now use this Persona as your camera feed for\\nany apps in Vision Pro that require a front facing\\ncamera, like FaceTime. And so I've tried, I've\\nbeen using FaceTime a few times in the Vision Pro, and it is, technically\\nspeaking, incredible. So I've made a few FaceTime\\ncalls in the past few days with some fellow reviewers, who you'll probably recognize\\nfrom their Personas, who are also testing the Vision Pro. And universally, once we\\nall got past the shock of, oh my god, it's you. It looks like a digital version of you. This is crazy. I've never seen anything like this before. Once we got past that, there\\nis a ton happening here. So you can see the FaceTime windows literally appear as just that. They're just like glassy\\nwindows floating in space with people looking through them. And then the angle that\\nyou look into the window is gonna match the angle that\\nthey see you looking at them. Meaning if we're all in\\nVision Pros on this call, unlikely, but hear me out. If we're all in Vision Pros, and you've got a bunch of\\npeople on this FaceTime call, so there's somebody to the left, and somebody to the right,\\nif I look to the person, and make eye contact with\\nthe person to the right, the person to the left\\nsees the side of my head, because I'm looking at somebody else. That's already pretty cool. And then the same thing\\nis true for hand gestures. So we tried this out. Turns out you can reach out and make hand gestures that\\nare tracked by the cameras in this bubble in front of you, and they show up at the correct angle towards the person that\\nyou're gesturing at, so not towards everybody else on the call. Oh wait, wait, wait. Okay, good test. So wait, Justine, do you see this? - Yes. - And Brian, do you see?\\n- I don't see that. I don't see that, Marques.\\n- Whoa. - Now wait. So now Brian, do you see this?\\n(Justine gasps) - Now I can see that, Marques. - And then on top of\\nthat, spatial audio here is incredibly well developed. So again, you're on the\\ncall, the voice of the person to the right comes from the right side. The voice of the person to\\nthe left comes from the left. But also, you can just pick\\nup and move the window around, and that angle will match where\\nthe people are in the room and where their sound\\nand video comes from. If I put you on the\\nother side of the room, it sounds like they're further away. And if I turn up the environment, and bring them into the\\nmoon, or some other 3D space, it actually sounds much more\\nlike I'm in a gigantic space with no echo, versus in the actual room. It's all very subtle,\\nbut very well considered. So once you're in this a while, you start to notice all\\nthese little smaller things. Again, it's not quite human-like. It's not like looking at a\\nvideo feed of a human face, but it is still, like\\nit has a lot of like, this would be the best avatar\\nanyone's ever made in 2K. Like no one's ever done a 2K face scan and had it look this good, but it's still not as\\ngood as a perfect reality. It's a, you've heard the\\nuncanny valley thing before. I think the number one\\nweakness for the avatars or the Personas that I've seen is hair. So basically everyone I've talked to has like a frozen lump of hair instead of flowing realistic hair. And that's true about all flowing things, like however your hair\\nwas when you did the scan, it's frozen that way. And so is any necklace you're wearing, whether it's crooked or not, or I guess, technically\\nalso any makeup you had on, or however you looked\\nwhen you did the scan. Maybe that could be a good thing. Maybe you did a scan when you\\nwere looking all dolled up, and then you get on a 7:00 AM call, and you still look perfect even though you look like you just\\nwoke up in real life. So I guess there's that too. But anyway, all that is to say FaceTime. FaceTime is the most well thought out, like most futuristic\\nVision Pro experience. It just is. So I'll end this video with this. Now you know what it's\\nlike to use and operate the Vision Pro. But there's still a lot more to consider when actually considering\\nif you should buy and own this thing, from the use cases, to the things that work\\nwell, and don't work well, the philosophy behind it, the\\nprices, all of that stuff. That's what's gonna be for my full review. Like there are parts of this thing that are absolutely amazing, unparalleled, best I've ever seen. But the reason it's so interesting is because it's actually a young category. Like we're so used to this\\nslow, boring iteration in mature categories, like\\nsmartphones, and laptops, and you always see the comments talking about how tech is so boring, but now they're actually\\njumping into something risky, and it's actually fun, and\\nthere is downfalls and flaws, and it's fun to actually\\nweigh the pros and cons. So I'll be expanding on all these way more in the full review, but\\nI'll leave you with this. I've got my upsides and\\ndownsides to Vision Pro. It's been a week. Upsides, some of the stuff\\nthat's the best I've ever seen in a headset. Immersion, placement\\nin space, eye tracking and hand control, passthrough, ecosystem, and spatial audio. And the downsides, weight and comfort, the eyes on the outside, app selection right now,\\nbattery life, and price. So the full reviews in the works. Definitely get subscribed\\nto be among the first to see that when it drops. Either way, till the next one. Thanks for watching. Catch you later. Peace. \""
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "5903dd1c-4087-4b6a-a097-1dceb055fae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"transcript.txt\") as f:\n",
    "    transcript = f.read()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "eaca2b35-d334-4eb8-a795-295a7821789a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "\n",
    "t1 = \"Data science is the study of data to extract meaningful insights for business. It is a multidisciplinary approach that combines principles and practices from the fields of mathematics, statistics, artificial intelligence, and computer engineering to analyze large amounts of data. This analysis helps data scientists to ask and answer questions like what happened, why it happened, what will happen, and what can be done with the results.\"\n",
    "\n",
    "t2 = \"Data science is important because it combines tools, methods, and technology to generate meaning from data. Modern organizations are inundated with data; there is a proliferation of devices that can automatically collect and store information. Online systems and payment portals capture more data in the fields of e-commerce, medicine, finance, and every other aspect of human life. We have text, audio, video, and image data available in vast quantities.\"\n",
    "\n",
    "t3 = \"Descriptive analysis examines data to gain insights into what happened or what is happening in the data environment. It is characterized by data visualizations such as pie charts, bar charts, line graphs, tables, or generated narratives. For example, a flight booking service may record data like the number of tickets booked each day. Descriptive analysis will reveal booking spikes, booking slumps, and high-performing months for this service.\"\n",
    "\n",
    "\n",
    "text_splitter = SemanticChunker(OpenAIEmbeddings())\n",
    "docs = text_splitter.create_documents([t1,t2,t3],[{\"start\":1,\"end\":2},{\"start\":3,\"end\":4},{\"start\":5,\"end\":6}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "2535d00f-f9e4-41e5-95e8-a8f3699d722b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Data science is the study of data to extract meaningful insights for business. It is a multidisciplinary approach that combines principles and practices from the fields of mathematics, statistics, artificial intelligence, and computer engineering to analyze large amounts of data.', metadata={'start': 1, 'end': 2}),\n",
       " Document(page_content='This analysis helps data scientists to ask and answer questions like what happened, why it happened, what will happen, and what can be done with the results.', metadata={'start': 1, 'end': 2}),\n",
       " Document(page_content='Data science is important because it combines tools, methods, and technology to generate meaning from data. Modern organizations are inundated with data; there is a proliferation of devices that can automatically collect and store information.', metadata={'start': 3, 'end': 4}),\n",
       " Document(page_content='Online systems and payment portals capture more data in the fields of e-commerce, medicine, finance, and every other aspect of human life. We have text, audio, video, and image data available in vast quantities.', metadata={'start': 3, 'end': 4}),\n",
       " Document(page_content='Descriptive analysis examines data to gain insights into what happened or what is happening in the data environment. It is characterized by data visualizations such as pie charts, bar charts, line graphs, tables, or generated narratives. For example, a flight booking service may record data like the number of tickets booked each day.', metadata={'start': 5, 'end': 6}),\n",
       " Document(page_content='Descriptive analysis will reveal booking spikes, booking slumps, and high-performing months for this service.', metadata={'start': 5, 'end': 6})]"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80a8c256-5d9c-41b2-9b20-925d5f983465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "webdev",
   "language": "python",
   "name": "insightflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
